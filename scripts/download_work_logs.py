"""Notion ì—…ë¬´ì¼ì§€ë¥¼ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional

import pytz
from dotenv import load_dotenv

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.notion.client import NotionClient
from src.common.notion_utils import extract_page_content

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# KST timezone
KST = pytz.timezone('Asia/Seoul')


async def download_work_logs(
    database_id: str,
    output_dir: str = "./work_logs_export",
    format: str = "markdown",
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
):
  """
  Notion ì—…ë¬´ì¼ì§€ë¥¼ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œ

  Args:
      database_id: Notion ë°ì´í„°ë² ì´ìŠ¤ ID
      output_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
      format: ì¶œë ¥ í˜•ì‹ (markdown, json, both)
      start_date: ì‹œì‘ì¼ (YYYY-MM-DD), Noneì´ë©´ ì „ì²´
      end_date: ì¢…ë£Œì¼ (YYYY-MM-DD), Noneì´ë©´ ì „ì²´
  """
  try:
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    logger.info(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_path.absolute()}")
    logger.info(f"ğŸ“ ì¶œë ¥ í˜•ì‹: {format}")

    # Initialize Notion client
    client = NotionClient()

    # Build filter
    filter_params = None
    if start_date or end_date:
      conditions = []
      if start_date:
        conditions.append({
          "property": "ì‘ì„±ì¼",
          "date": {"on_or_after": start_date}
        })
      if end_date:
        conditions.append({
          "property": "ì‘ì„±ì¼",
          "date": {"on_or_before": end_date}
        })

      if len(conditions) == 1:
        filter_params = conditions[0]
      else:
        filter_params = {"and": conditions}

    # Query database
    logger.info(f"ğŸ” ì—…ë¬´ì¼ì§€ ì¡°íšŒ ì¤‘... (DB: {database_id})")
    pages = await client.query_database(
        database_id=database_id,
        filter_params=filter_params,
        sorts=[{"property": "ì‘ì„±ì¼", "direction": "ascending"}]
    )

    if not pages:
      logger.info("ğŸ“­ ì¡°íšŒëœ ì—…ë¬´ì¼ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
      return

    logger.info(f"âœ… ì´ {len(pages)}ê°œì˜ ì—…ë¬´ì¼ì§€ ë°œê²¬")

    # Download each page
    downloaded = 0
    failed = 0

    for i, page in enumerate(pages, 1):
      page_id = page["id"]
      properties = page.get("properties", {})

      # Extract metadata
      title_prop = properties.get("title") or properties.get("Title") or properties.get("ì œëª©", {})
      title = ""
      if title_prop.get("title"):
        title = "".join([t.get("plain_text", "") for t in title_prop["title"]])

      date_prop = properties.get("ì‘ì„±ì¼", {})
      date = ""
      if date_prop.get("date"):
        date = date_prop["date"].get("start", "")

      logger.info(f"ğŸ“„ [{i}/{len(pages)}] {date} - {title[:50]}...")

      try:
        # Get page content
        content = await extract_page_content(client, page_id, format="markdown")

        # Prepare metadata
        metadata = {
          "page_id": page_id,
          "title": title,
          "date": date,
          "url": f"https://notion.so/{page_id.replace('-', '')}",
          "downloaded_at": datetime.now(KST).isoformat(),
        }

        # Extract additional properties
        for prop_name, prop_value in properties.items():
          if prop_name in ["title", "Title", "ì œëª©", "ì‘ì„±ì¼"]:
            continue

          prop_type = prop_value.get("type")

          if prop_type == "select":
            select_value = prop_value.get("select")
            if select_value:
              metadata[prop_name] = select_value.get("name", "")

          elif prop_type == "multi_select":
            multi_select_values = prop_value.get("multi_select", [])
            metadata[prop_name] = [v.get("name", "") for v in multi_select_values]

          elif prop_type == "rich_text":
            rich_text = prop_value.get("rich_text", [])
            metadata[prop_name] = "".join([t.get("plain_text", "") for t in rich_text])

        # Create filename
        safe_date = date.replace("-", "") if date else "unknown"
        safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_'))[:50]
        safe_title = safe_title.strip() or "untitled"
        base_filename = f"{safe_date}_{safe_title}"

        # Save markdown
        if format in ["markdown", "both"]:
          md_file = output_path / f"{base_filename}.md"
          with open(md_file, "w", encoding="utf-8") as f:
            # Write frontmatter
            f.write("---\n")
            f.write(f"title: {title}\n")
            f.write(f"date: {date}\n")
            f.write(f"page_id: {page_id}\n")
            f.write(f"url: {metadata['url']}\n")
            f.write("---\n\n")
            # Write content
            f.write(content)
          logger.info(f"  âœ… ë§ˆí¬ë‹¤ìš´ ì €ì¥: {md_file.name}")

        # Save JSON
        if format in ["json", "both"]:
          json_file = output_path / f"{base_filename}.json"
          with open(json_file, "w", encoding="utf-8") as f:
            json.dump({
              "metadata": metadata,
              "content": content,
              "properties": properties
            }, f, ensure_ascii=False, indent=2)
          logger.info(f"  âœ… JSON ì €ì¥: {json_file.name}")

        downloaded += 1

      except Exception as e:
        logger.error(f"  âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        failed += 1

    # Save index
    logger.info("\nğŸ“‘ ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„± ì¤‘...")
    index = {
      "total": len(pages),
      "downloaded": downloaded,
      "failed": failed,
      "download_date": datetime.now(KST).isoformat(),
      "database_id": database_id,
      "date_range": {
        "start": start_date,
        "end": end_date
      },
      "pages": []
    }

    for page in pages:
      page_id = page["id"]
      properties = page.get("properties", {})

      title_prop = properties.get("title") or properties.get("Title") or properties.get("ì œëª©", {})
      title = ""
      if title_prop.get("title"):
        title = "".join([t.get("plain_text", "") for t in title_prop["title"]])

      date_prop = properties.get("ì‘ì„±ì¼", {})
      date = ""
      if date_prop.get("date"):
        date = date_prop["date"].get("start", "")

      index["pages"].append({
        "page_id": page_id,
        "title": title,
        "date": date,
        "url": f"https://notion.so/{page_id.replace('-', '')}"
      })

    index_file = output_path / "index.json"
    with open(index_file, "w", encoding="utf-8") as f:
      json.dump(index, f, ensure_ascii=False, indent=2)

    logger.info(f"âœ… ì¸ë±ìŠ¤ ì €ì¥: {index_file.name}")

    # Summary
    print("\n" + "=" * 80)
    print("âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nğŸ“Š ì´ ì—…ë¬´ì¼ì§€: {len(pages)}ê°œ")
    print(f"âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {downloaded}ê°œ")
    print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {failed}ê°œ")
    print(f"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {output_path.absolute()}")
    print(f"ğŸ“ ì¶œë ¥ í˜•ì‹: {format}")
    print("\n" + "=" * 80 + "\n")

  except Exception as e:
    logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)


async def main():
  """ë©”ì¸ í•¨ìˆ˜"""
  try:
    load_dotenv()

    # Get DB IDs from environment
    user_db_mapping_str = os.getenv("NOTION_USER_DATABASE_MAPPING", "{}")

    if not user_db_mapping_str or user_db_mapping_str == "{}":
      logger.error("âŒ NOTION_USER_DATABASE_MAPPING í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
      logger.info("í™˜ê²½ ë³€ìˆ˜ í˜•ì‹:")
      logger.info('{"USER_ID":{"alias":"í™ê¸¸ë™","work_log_db":"DB_ID"}}')
      return

    try:
      user_db_mapping = json.loads(user_db_mapping_str)
    except json.JSONDecodeError as e:
      logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
      return

    # Get first user's DB IDs
    if not user_db_mapping:
      logger.error("âŒ ë°ì´í„°ë² ì´ìŠ¤ ë§¤í•‘ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
      return

    user_id = list(user_db_mapping.keys())[0]
    user_dbs = user_db_mapping[user_id]

    user_alias = user_dbs.get("alias", "ì´ë¦„ì—†ìŒ")
    work_log_db_id = user_dbs.get("work_log_db")

    if not work_log_db_id:
      logger.error("âŒ work_log_db IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
      return

    logger.info(f"âœ… DB ì„¤ì • í™•ì¸ ì™„ë£Œ")
    logger.info(f"  User: {user_alias} ({user_id})")
    logger.info(f"  Work Log DB: {work_log_db_id}")

    # Check if running interactively
    is_interactive = sys.stdin.isatty()

    print("\n" + "=" * 80)
    print("Notion ì—…ë¬´ì¼ì§€ ë‹¤ìš´ë¡œë“œ")
    print("=" * 80)

    if is_interactive:
      # Interactive mode
      output_dir = input("\nì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ./work_logs_export): ").strip() or "./work_logs_export"

      print("\nì¶œë ¥ í˜•ì‹ ì„ íƒ:")
      print("  1. markdown - ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥")
      print("  2. json - JSON íŒŒì¼ë¡œ ì €ì¥ (ë©”íƒ€ë°ì´í„° í¬í•¨)")
      print("  3. both - ë§ˆí¬ë‹¤ìš´ + JSON ëª¨ë‘ ì €ì¥")
      format_choice = input("ì„ íƒ (ê¸°ë³¸ê°’: markdown): ").strip() or "1"

      format_map = {"1": "markdown", "2": "json", "3": "both"}
      format = format_map.get(format_choice, "markdown")

      print("\në‚ ì§œ ë²”ìœ„ ì„ íƒ (ì „ì²´ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´ Enter):")
      start_date = input("ì‹œì‘ì¼ (YYYY-MM-DD): ").strip() or None
      end_date = input("ì¢…ë£Œì¼ (YYYY-MM-DD): ").strip() or None

    else:
      # Non-interactive mode: use command-line args
      output_dir = sys.argv[1] if len(sys.argv) > 1 else "./work_logs_export"
      format = sys.argv[2] if len(sys.argv) > 2 else "markdown"
      start_date = sys.argv[3] if len(sys.argv) > 3 else None
      end_date = sys.argv[4] if len(sys.argv) > 4 else None

      logger.info(f"ğŸ¤– Non-interactive mode detected")
      logger.info(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
      logger.info(f"  ì¶œë ¥ í˜•ì‹: {format}")
      if start_date:
        logger.info(f"  ì‹œì‘ì¼: {start_date}")
      if end_date:
        logger.info(f"  ì¢…ë£Œì¼: {end_date}")

    print("\n" + "=" * 80)
    logger.info(f"ğŸš€ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    print("=" * 80 + "\n")

    # Download
    await download_work_logs(
        database_id=work_log_db_id,
        output_dir=output_dir,
        format=format,
        start_date=start_date,
        end_date=end_date
    )

  except Exception as e:
    logger.error(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}", exc_info=True)


if __name__ == "__main__":
  asyncio.run(main())
