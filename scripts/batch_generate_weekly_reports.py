"""
ì£¼ê°„ ë¦¬í¬íŠ¸ ë°°ì¹˜ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

ì—…ë¬´ì¼ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ëˆ„ë½ëœ ì£¼ê°„ ë¦¬í¬íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
5ê°œì”© ë¹„ë™ê¸°ë¡œ ë™ì‹œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple

import pytz
from dotenv import load_dotenv

# Load .env file from project root
project_root = Path(__file__).parent.parent
env_file = project_root / '.env'
if env_file.exists():
  load_dotenv(env_file)
  print(f"âœ… Loaded environment from {env_file}")
else:
  print(f"âš ï¸ No .env file found at {env_file}")

# Add project root to path
sys.path.insert(0, str(project_root))

from src.common.date_utils import get_week_info, format_week_string
from src.common.notion_utils import get_user_database_mapping
from src.notion.client import NotionClient
from src.notion.weekly_report_agent import get_weekly_report_manager

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

KST = pytz.timezone('Asia/Seoul')

# Configuration
BATCH_SIZE = 5  # ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜
AI_PROVIDER = "claude"  # ì‚¬ìš©í•  AI ì œê³µì


class WeeklyReportBatchGenerator:
  """ì£¼ê°„ ë¦¬í¬íŠ¸ ë°°ì¹˜ ìƒì„±ê¸°"""

  def __init__(
      self,
      user_id: str,
      ai_provider: str = "claude",
      batch_size: int = 5
  ):
    """
    Initialize batch generator

    Args:
        user_id: Slack user ID
        ai_provider: AI provider type
        batch_size: ë™ì‹œ ì²˜ë¦¬í•  ì£¼ê°„ ë¦¬í¬íŠ¸ ê°œìˆ˜
    """
    self.user_id = user_id
    self.ai_provider = ai_provider
    self.batch_size = batch_size
    self.notion_client = NotionClient()
    self.weekly_manager = get_weekly_report_manager(ai_provider)

    # Get user database mappings
    user_dbs = get_user_database_mapping(user_id)
    if not user_dbs:
      raise ValueError(f"No database mapping found for user: {user_id}")

    self.work_log_db_id = user_dbs.get("work_log_db")
    self.weekly_report_db_id = user_dbs.get("weekly_report_db")

    if not self.work_log_db_id or not self.weekly_report_db_id:
      raise ValueError(f"Incomplete database mapping for user: {user_id}")

    logger.info(
        f"âœ… Initialized for user {user_id} (AI: {ai_provider}, "
        f"batch_size: {batch_size})"
    )

  async def get_work_log_dates(self) -> List[str]:
    """
    ì—…ë¬´ì¼ì§€ DBì—ì„œ ëª¨ë“  ì‘ì„±ì¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Returns:
        ë‚ ì§œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ (YYYY-MM-DD)
    """
    logger.info("ğŸ“… ì—…ë¬´ì¼ì§€ ë‚ ì§œ ì¡°íšŒ ì¤‘...")

    try:
      results = await self.notion_client.query_database(
          database_id=self.work_log_db_id,
          filter_params=None
      )

      dates = []
      for page in results:
        properties = page.get("properties", {})
        date_prop = properties.get("ì‘ì„±ì¼", {}).get("date", {})

        if date_prop:
          date_str = date_prop.get("start")
          if date_str:
            dates.append(date_str)

      # Sort dates in Python
      dates.sort()

      logger.info(f"âœ… ì´ {len(dates)}ê°œì˜ ì—…ë¬´ì¼ì§€ ë°œê²¬")
      return dates

    except Exception as e:
      logger.error(f"âŒ ì—…ë¬´ì¼ì§€ ë‚ ì§œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
      raise

  async def get_existing_weekly_reports(self) -> Set[str]:
    """
    ì´ë¯¸ ìƒì„±ëœ ì£¼ê°„ ë¦¬í¬íŠ¸ì˜ ì£¼ì°¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Returns:
        ì£¼ì°¨ ë¬¸ìì—´ ì§‘í•© (ì˜ˆ: {"2025-W03", "2025-W04"})
    """
    logger.info("ğŸ“Š ê¸°ì¡´ ì£¼ê°„ ë¦¬í¬íŠ¸ ì¡°íšŒ ì¤‘...")

    try:
      results = await self.notion_client.query_database(
          database_id=self.weekly_report_db_id,
          filter_params=None
      )

      existing_weeks = set()
      for page in results:
        properties = page.get("properties", {})

        # ì£¼ì°¨ ì†ì„±ì—ì„œ ì¶”ì¶œ
        week_prop = properties.get("ì£¼ì°¨", {})
        title_parts = week_prop.get("title", [])

        if title_parts:
          week_str = title_parts[0].get("plain_text", "")
          if week_str and "-W" in week_str:
            existing_weeks.add(week_str)

      logger.info(f"âœ… ê¸°ì¡´ ì£¼ê°„ ë¦¬í¬íŠ¸: {len(existing_weeks)}ê°œ")
      return existing_weeks

    except Exception as e:
      logger.error(f"âŒ ì£¼ê°„ ë¦¬í¬íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
      raise

  def group_dates_by_week(self, dates: List[str]) -> Dict[str, List[str]]:
    """
    ë‚ ì§œë¥¼ ì£¼ì°¨ë³„ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.

    Args:
        dates: ë‚ ì§œ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì£¼ì°¨ë³„ ë‚ ì§œ ë”•ì…”ë„ˆë¦¬
    """
    weeks = {}

    for date_str in dates:
      try:
        date = datetime.fromisoformat(date_str)
        year, week = get_week_info(date)
        week_key = format_week_string(year, week)

        if week_key not in weeks:
          weeks[week_key] = []
        weeks[week_key].append(date_str)

      except (ValueError, IndexError):
        logger.warning(f"âš ï¸ Invalid date format: {date_str}")
        continue

    return weeks

  async def generate_weekly_report(
      self,
      year: int,
      week: int,
      semaphore: asyncio.Semaphore
  ) -> Tuple[str, bool, str]:
    """
    ì£¼ê°„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ë™ì‹œ ì‹¤í–‰ ì œí•œ ì ìš©).

    Args:
        year: ì—°ë„
        week: ì£¼ì°¨
        semaphore: ë™ì‹œ ì‹¤í–‰ ì œí•œìš© Semaphore

    Returns:
        (week_str, success, message) íŠœí”Œ
    """
    week_str = format_week_string(year, week)

    async with semaphore:
      logger.info(f"ğŸ”„ [{week_str}] ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")

      try:
        result = await self.weekly_manager.generate_weekly_report(
            year=year,
            week=week,
            work_log_database_id=self.work_log_db_id,
            weekly_report_database_id=self.weekly_report_db_id,
            progress_callback=None  # ë°°ì¹˜ ëª¨ë“œì—ì„œëŠ” progress ì½œë°± ì—†ìŒ
        )

        page_url = result.get('page_url', '')
        daily_logs_count = result.get('daily_logs_count', 0)

        logger.info(
            f"âœ… [{week_str}] ì™„ë£Œ! (ì—…ë¬´ì¼ì§€: {daily_logs_count}ê°œ)"
        )

        return week_str, True, page_url

      except ValueError as e:
        # ì—…ë¬´ì¼ì§€ê°€ ì—†ëŠ” ê²½ìš° ë“±
        logger.warning(f"âš ï¸ [{week_str}] ê±´ë„ˆëœ€: {e}")
        return week_str, False, str(e)

      except Exception as e:
        logger.error(f"âŒ [{week_str}] ì‹¤íŒ¨: {e}")
        return week_str, False, str(e)

  async def generate_missing_reports(
      self,
      missing_weeks: List[Tuple[int, int]]
  ) -> Dict[str, any]:
    """
    ëˆ„ë½ëœ ì£¼ê°„ ë¦¬í¬íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        missing_weeks: (year, week) íŠœí”Œ ë¦¬ìŠ¤íŠ¸

    Returns:
        ìƒì„± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if not missing_weeks:
      logger.info("âœ… ìƒì„±í•  ì£¼ê°„ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
      return {
        "total": 0,
        "success": 0,
        "failed": 0,
        "skipped": 0,
        "results": []
      }

    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“¦ ë°°ì¹˜ ìƒì„± ì‹œì‘: {len(missing_weeks)}ê°œ ì£¼ì°¨")
    logger.info(f"âš™ï¸ ë™ì‹œ ì²˜ë¦¬: {self.batch_size}ê°œ")
    logger.info(f"ğŸ¤– AI: {self.ai_provider}")
    logger.info(f"{'='*60}\n")

    # Semaphore for limiting concurrent tasks
    semaphore = asyncio.Semaphore(self.batch_size)

    # Create tasks
    tasks = []
    for year, week in missing_weeks:
      task = self.generate_weekly_report(year, week, semaphore)
      tasks.append(task)

    # Execute all tasks
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Analyze results
    success_count = 0
    failed_count = 0
    skipped_count = 0
    details = []

    for result in results:
      if isinstance(result, Exception):
        failed_count += 1
        details.append({
          "week": "unknown",
          "success": False,
          "message": str(result)
        })
      else:
        week_str, success, message = result
        if success:
          success_count += 1
        elif "ê±´ë„ˆëœ€" in message or "ì—†ìŒ" in message:
          skipped_count += 1
        else:
          failed_count += 1

        details.append({
          "week": week_str,
          "success": success,
          "message": message
        })

    logger.info(f"\n{'='*60}")
    logger.info(f"âœ… ë°°ì¹˜ ìƒì„± ì™„ë£Œ!")
    logger.info(f"{'='*60}")
    logger.info(f"ğŸ“Š ì´ ì‹œë„: {len(missing_weeks)}ê°œ")
    logger.info(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    logger.info(f"âš ï¸ ê±´ë„ˆëœ€: {skipped_count}ê°œ")
    logger.info(f"âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
    logger.info(f"{'='*60}\n")

    return {
      "total": len(missing_weeks),
      "success": success_count,
      "failed": failed_count,
      "skipped": skipped_count,
      "results": details
    }

  async def run(self) -> Dict[str, any]:
    """
    ë°°ì¹˜ ìƒì„± í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Returns:
        ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
      # 1. Get all work log dates
      work_log_dates = await self.get_work_log_dates()

      if not work_log_dates:
        logger.info("âš ï¸ ì—…ë¬´ì¼ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"total": 0, "success": 0, "failed": 0, "skipped": 0}

      # 2. Group by week
      weeks_with_logs = self.group_dates_by_week(work_log_dates)
      logger.info(f"ğŸ“… ì—…ë¬´ì¼ì§€ê°€ ìˆëŠ” ì£¼ì°¨: {len(weeks_with_logs)}ê°œ")

      # 3. Get existing reports
      existing_reports = await self.get_existing_weekly_reports()

      # 4. Find missing weeks
      missing_week_strs = set(weeks_with_logs.keys()) - existing_reports
      missing_weeks = []

      for week_str in sorted(missing_week_strs):
        try:
          year, week = map(int, week_str.replace('W', '-').split('-')[0:3:2])
          missing_weeks.append((year, week))
        except (ValueError, IndexError):
          continue

      logger.info(f"ğŸ” ìƒì„±í•  ì£¼ê°„ ë¦¬í¬íŠ¸: {len(missing_weeks)}ê°œ")

      if missing_weeks:
        # Show missing weeks
        logger.info("\nğŸ“‹ ìƒì„± ëŒ€ìƒ ì£¼ì°¨:")
        for year, week in sorted(missing_weeks):
          week_str = format_week_string(year, week)
          dates_count = len(weeks_with_logs.get(week_str, []))
          logger.info(f"  â€¢ {week_str} (ì—…ë¬´ì¼ì§€ {dates_count}ê°œ)")

      # 5. Generate missing reports
      result = await self.generate_missing_reports(missing_weeks)

      return result

    except Exception as e:
      logger.error(f"âŒ ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
      raise


async def main():
  """ë©”ì¸ í•¨ìˆ˜"""
  import argparse

  parser = argparse.ArgumentParser(
      description="ì£¼ê°„ ë¦¬í¬íŠ¸ ë°°ì¹˜ ìƒì„±"
  )
  parser.add_argument(
      "--user-id",
      type=str,
      help="Slack User ID (í™˜ê²½ë³€ìˆ˜ NOTION_USER_DATABASE_MAPPINGì—ì„œ ìë™ íƒì§€ ê°€ëŠ¥)"
  )
  parser.add_argument(
      "--ai-provider",
      type=str,
      default="claude",
      choices=["gemini", "claude", "ollama"],
      help="AI ì œê³µì (ê¸°ë³¸ê°’: claude)"
  )
  parser.add_argument(
      "--batch-size",
      type=int,
      default=5,
      help="ë™ì‹œ ì²˜ë¦¬ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)"
  )
  parser.add_argument(
      "--all-users",
      action="store_true",
      help="ëª¨ë“  ìœ ì €ì— ëŒ€í•´ ë°°ì¹˜ ìƒì„±"
  )

  args = parser.parse_args()

  # Get user IDs
  user_ids = []

  if args.all_users:
    # Get all user IDs from environment
    user_db_mapping_str = os.getenv("NOTION_USER_DATABASE_MAPPING", "{}")
    try:
      user_db_mapping = json.loads(user_db_mapping_str)
      user_ids = list(user_db_mapping.keys())
      logger.info(f"âœ… {len(user_ids)}ëª…ì˜ ìœ ì € ë°œê²¬")
    except json.JSONDecodeError:
      logger.error("âŒ NOTION_USER_DATABASE_MAPPING íŒŒì‹± ì‹¤íŒ¨")
      sys.exit(1)

  elif args.user_id:
    user_ids = [args.user_id]

  else:
    # Try to get first user from environment
    user_db_mapping_str = os.getenv("NOTION_USER_DATABASE_MAPPING", "{}")
    try:
      user_db_mapping = json.loads(user_db_mapping_str)
      if user_db_mapping:
        user_ids = [list(user_db_mapping.keys())[0]]
        logger.info(f"âœ… ì²« ë²ˆì§¸ ìœ ì € ì‚¬ìš©: {user_ids[0]}")
      else:
        logger.error("âŒ ìœ ì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --user-id ë˜ëŠ” --all-users ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(1)
    except json.JSONDecodeError:
      logger.error("âŒ NOTION_USER_DATABASE_MAPPING íŒŒì‹± ì‹¤íŒ¨")
      sys.exit(1)

  # Process each user
  for user_id in user_ids:
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ‘¤ ìœ ì €: {user_id}")
    logger.info(f"{'='*60}\n")

    try:
      generator = WeeklyReportBatchGenerator(
          user_id=user_id,
          ai_provider=args.ai_provider,
          batch_size=args.batch_size
      )

      result = await generator.run()

      # Print summary
      logger.info(f"\n{'='*60}")
      logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼ (ìœ ì €: {user_id})")
      logger.info(f"{'='*60}")
      logger.info(f"ì´ ì‹œë„: {result['total']}ê°œ")
      logger.info(f"âœ… ì„±ê³µ: {result['success']}ê°œ")
      logger.info(f"âš ï¸ ê±´ë„ˆëœ€: {result['skipped']}ê°œ")
      logger.info(f"âŒ ì‹¤íŒ¨: {result['failed']}ê°œ")
      logger.info(f"{'='*60}\n")

    except Exception as e:
      logger.error(f"âŒ ìœ ì € {user_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
      continue

  logger.info("âœ… ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")


if __name__ == "__main__":
  asyncio.run(main())
