"""ì›”ê°„ ë¦¬í¬íŠ¸ ë¶„ì„ê¸°"""

import logging
import os
from typing import Dict, List, Optional

from ..ai import generate_with_gemini_fallback

logger = logging.getLogger(__name__)


class MonthlyAnalyzer:
  """ì›”ê°„ ì£¼ê°„ ë¦¬í¬íŠ¸ ì¢…í•© ë¶„ì„ê¸°"""

  def __init__(self, ai_provider_type: str = "claude"):
    """
    Initialize MonthlyAnalyzer

    Args:
        ai_provider_type: AI provider type (gemini, claude, ollama)
    """
    self.ai_provider_type = ai_provider_type
    self.last_used_ai_provider: Optional[str] = None

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt_path = os.path.join(
        os.path.dirname(os.path.dirname(__file__)),
        "prompts",
        "monthly_report_analysis.txt"
    )
    with open(prompt_path, "r", encoding="utf-8") as f:
      self.prompt_template = f.read()

    logger.info(f"âœ… MonthlyAnalyzer initialized (AI: {ai_provider_type})")

  async def analyze_monthly_reports(
      self,
      weekly_reports: List[Dict],
      notion_client,
      resume_page_id: Optional[str] = None
  ) -> str:
    """
    ì›”ê°„ ì£¼ê°„ ë¦¬í¬íŠ¸ ì¢…í•© ë¶„ì„

    Args:
        weekly_reports: ì£¼ê°„ ë¦¬í¬íŠ¸ íŽ˜ì´ì§€ ëª©ë¡ (Notion API ì‘ë‹µ)
        notion_client: NotionClient ì¸ìŠ¤í„´ìŠ¤
        resume_page_id: ì´ë ¥ì„œ íŽ˜ì´ì§€ ID (ì„ íƒ)

    Returns:
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¶„ì„ ê²°ê³¼
    """
    logger.info(f"ðŸ“Š ì›”ê°„ ë¶„ì„ ì‹œìž‘: {len(weekly_reports)}ê°œ ì£¼ê°„ ë¦¬í¬íŠ¸")

    # 1. ê° ì£¼ê°„ ë¦¬í¬íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„° ë° ì½˜í…ì¸  ì¶”ì¶œ
    weekly_data = []
    for page in weekly_reports:
      metadata = self.extract_weekly_report_metadata(page)
      content = await self.get_page_content(metadata["page_id"], notion_client)

      # ì£¼ì°¨ ì •ë³´ì™€ ì½˜í…ì¸  ê²°í•©
      report_text = f"## {metadata['week']} ({metadata['start_date']} ~ {metadata['end_date']})\n\n{content}"
      weekly_data.append(report_text.strip())

    # 2. ëª¨ë“  ì£¼ê°„ ë¦¬í¬íŠ¸ ê²°í•©
    combined_reports = "\n\n---\n\n".join(weekly_data)

    # 3. ì´ë ¥ì„œ ë‚´ìš© ì½ê¸° (ìžˆëŠ” ê²½ìš°)
    resume_content = ""
    if resume_page_id:
      try:
        logger.info(f"ðŸ“„ ì´ë ¥ì„œ íŽ˜ì´ì§€ ì½ê¸°: {resume_page_id}")
        resume_content = await self.get_page_content(resume_page_id, notion_client)
        if resume_content:
          logger.info(f"âœ… ì´ë ¥ì„œ ë‚´ìš© ë¡œë“œ ì™„ë£Œ ({len(resume_content)}ìž)")
        else:
          logger.warning("âš ï¸ ì´ë ¥ì„œ íŽ˜ì´ì§€ê°€ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤")
      except Exception as e:
        logger.warning(f"âš ï¸ ì´ë ¥ì„œ ì½ê¸° ì‹¤íŒ¨ (ì„ íƒì‚¬í•­): {e}")
        resume_content = ""

    # 4. AI ë¶„ì„ ìš”ì²­
    prompt = self.prompt_template.replace("{weekly_reports}", combined_reports)
    prompt = prompt.replace("{resume_content}", resume_content if resume_content else "(ì´ë ¥ì„œ ì •ë³´ ì—†ìŒ)")

    logger.info(f"ðŸ¤– AI ë¶„ì„ ì‹œìž‘... (ë‚´ìš© ê¸¸ì´: {len(combined_reports)}ìž)")

    analysis_text, used_provider = await generate_with_gemini_fallback(
        self.ai_provider_type,
        prompt=prompt,
        system_prompt="""ë‹¹ì‹ ì€ ì»¤ë¦¬ì–´ ì½”ì¹˜ì´ìž ì´ë ¥ì„œ ìž‘ì„± ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.

ì£¼ê°„ ë¦¬í¬íŠ¸ ê°œìˆ˜ì™€ ê´€ê³„ì—†ì´ ì œê³µëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›”ê°„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ì¤‘ìš”**:
- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”
- í”„ë¡¬í”„íŠ¸ì˜ ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ë¥¼ ì •í™•ížˆ ë”°ë¥´ì„¸ìš”
- "ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤" ê°™ì€ ì‘ë‹µì€ ê¸ˆì§€ìž…ë‹ˆë‹¤
- ì£¼ê°„ ë¦¬í¬íŠ¸ê°€ 1ê°œë§Œ ìžˆì–´ë„ í•´ë‹¹ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì„ ì˜ ì›”ê°„ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”"""
    )

    self.last_used_ai_provider = used_provider
    logger.info(f"âœ… AI ë¶„ì„ ì™„ë£Œ (ì œê³µìž: {used_provider})")

    # 4. ë§ˆí¬ë‹¤ìš´ ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
    import re
    # ```markdown ... ``` ë¸”ë¡ì´ ìžˆìœ¼ë©´ ì¶”ì¶œ
    markdown_match = re.search(r'```markdown\s*\n(.*?)```', analysis_text, re.DOTALL)
    if markdown_match:
      analysis_text = markdown_match.group(1).strip()
    else:
      # ì¼ë°˜ ``` ... ``` ë¸”ë¡ì´ ìžˆìœ¼ë©´ ì¶”ì¶œ
      code_match = re.search(r'```\s*\n(.*?)```', analysis_text, re.DOTALL)
      if code_match:
        analysis_text = code_match.group(1).strip()

    logger.info("ðŸ“‹ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ ì™„ë£Œ")
    return analysis_text.strip()

  def extract_weekly_report_metadata(self, page: Dict) -> Dict:
    """
    ì£¼ê°„ ë¦¬í¬íŠ¸ íŽ˜ì´ì§€ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

    Args:
        page: Notion íŽ˜ì´ì§€ ê°ì²´

    Returns:
        ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    properties = page.get("properties", {})

    # ì£¼ì°¨ (Title)
    week_prop = properties.get("ì£¼ì°¨", {})
    week_title = week_prop.get("title", [])
    week = week_title[0]["text"]["content"] if week_title else "Unknown"

    # ì‹œìž‘ì¼
    start_date_prop = properties.get("ì‹œìž‘ì¼", {})
    start_date_obj = start_date_prop.get("date", {})
    start_date = start_date_obj.get("start", "") if start_date_obj else ""

    # ì¢…ë£Œì¼
    end_date_prop = properties.get("ì¢…ë£Œì¼", {})
    end_date_obj = end_date_prop.get("date", {})
    end_date = end_date_obj.get("start", "") if end_date_obj else ""

    return {
      "page_id": page["id"],
      "week": week,
      "start_date": start_date,
      "end_date": end_date
    }

  async def get_page_content(self, page_id: str, notion_client) -> str:
    """
    Notion íŽ˜ì´ì§€ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°

    Args:
        page_id: Notion íŽ˜ì´ì§€ ID
        notion_client: NotionClient ì¸ìŠ¤í„´ìŠ¤

    Returns:
        íŽ˜ì´ì§€ ì½˜í…ì¸  (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
    """
    try:
      blocks = await notion_client.client.blocks.children.list(block_id=page_id)
      content_parts = []

      for block in blocks.get("results", []):
        block_type = block.get("type")

        if block_type == "paragraph":
          rich_text = block.get("paragraph", {}).get("rich_text", [])
          text = "".join([t.get("plain_text", "") for t in rich_text])
          if text.strip():
            content_parts.append(text)

        elif block_type == "heading_1":
          rich_text = block.get("heading_1", {}).get("rich_text", [])
          text = "".join([t.get("plain_text", "") for t in rich_text])
          content_parts.append(f"# {text}")

        elif block_type == "heading_2":
          rich_text = block.get("heading_2", {}).get("rich_text", [])
          text = "".join([t.get("plain_text", "") for t in rich_text])
          content_parts.append(f"## {text}")

        elif block_type == "heading_3":
          rich_text = block.get("heading_3", {}).get("rich_text", [])
          text = "".join([t.get("plain_text", "") for t in rich_text])
          content_parts.append(f"### {text}")

        elif block_type == "bulleted_list_item":
          rich_text = block.get("bulleted_list_item", {}).get("rich_text", [])
          text = "".join([t.get("plain_text", "") for t in rich_text])
          content_parts.append(f"- {text}")

        elif block_type == "numbered_list_item":
          rich_text = block.get("numbered_list_item", {}).get("rich_text", [])
          text = "".join([t.get("plain_text", "") for t in rich_text])
          content_parts.append(f"1. {text}")

      return "\n\n".join(content_parts)

    except Exception as e:
      logger.error(f"âŒ íŽ˜ì´ì§€ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
      return ""


# Singleton instance
_monthly_analyzer = None


def get_monthly_analyzer(ai_provider_type: str = "claude") -> MonthlyAnalyzer:
  """
  Get or create singleton MonthlyAnalyzer instance

  Args:
      ai_provider_type: AI provider type (gemini, claude, ollama)

  Returns:
      MonthlyAnalyzer instance
  """
  global _monthly_analyzer
  if _monthly_analyzer is None or _monthly_analyzer.ai_provider_type != ai_provider_type:
    _monthly_analyzer = MonthlyAnalyzer(ai_provider_type=ai_provider_type)
  return _monthly_analyzer
