"""í† ìŠ¤ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼ (Playwright ê¸°ë°˜) - ë™ì  job_id íƒìƒ‰"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Optional

from playwright.async_api import async_playwright, Page, Browser

from .models import JobRequirement, ScrapedData, PositionCategory, TossJobCategory

logger = logging.getLogger(__name__)


@dataclass
class JobListItem:
    """ì±„ìš© ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ê³µê³  ì •ë³´"""
    job_id: str
    title: str
    tags: str  # íƒœê·¸ ë¬¸ìì—´ (ì§êµ° ë¶„ë¥˜ìš©)
    companies: list[str]  # ê³„ì—´ì‚¬ ëª©ë¡


class TossJobScraper:
    """í† ìŠ¤ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼ - ë™ì  job_id íƒìƒ‰"""

    BASE_URL = "https://toss.im/career/jobs"
    JOB_DETAIL_URL = "https://toss.im/career/job-detail"

    # ì§êµ°ë³„ í‚¤ì›Œë“œ ë§¤í•‘ (ì œëª©ê³¼ íƒœê·¸ì—ì„œ ë§¤ì¹­)
    CATEGORY_KEYWORDS: dict[TossJobCategory, list[str]] = {
        TossJobCategory.BACKEND: [
            "server developer", "backend", "node.js developer", "python developer",
            "ì„œë²„ ê°œë°œ", "ë°±ì—”ë“œ", "server", "java developer", "go developer",
        ],
        TossJobCategory.FRONTEND: [
            "frontend developer", "frontend engineer", "frontend ux",
            "í”„ë¡ íŠ¸ì—”ë“œ", "frontend ops", "frontend platform",
        ],
        TossJobCategory.APP: [
            "ios developer", "android developer", "android platform", "ios platform",
            "ios engineer", "android engineer", "ì•± ê°œë°œ", "ëª¨ë°”ì¼",
        ],
        TossJobCategory.INFRA: [
            "devops", "sre", "site reliability", "system engineer", "cloud engineer",
            "infrastructure", "ì¸í”„ë¼", "ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´", "network engineer",
        ],
        TossJobCategory.QA: [
            "qa engineer", "qa manager", "qa specialist", "test automation",
            "quality assurance", "í…ŒìŠ¤íŠ¸", "í’ˆì§ˆ",
        ],
        TossJobCategory.DEVICE: [
            "device software", "embedded", "ì„ë² ë””ë“œ", "device engineer",
        ],
        TossJobCategory.FULLSTACK: [
            "full stack", "fullstack", "í’€ìŠ¤íƒ",
        ],
        TossJobCategory.MILITARY: [
            "ì‚°ì—…ê¸°ëŠ¥ìš”ì›", "ì „ë¬¸ì—°êµ¬ìš”ì›", "ë³‘ì—­íŠ¹ë¡€",
        ],
    }

    def __init__(self, data_dir: str = "data/resume_evaluator"):
        """
        Args:
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.scraped_data_path = self.data_dir / "scraped_positions.json"
        self._job_list_cache: dict[str, list[JobListItem]] = {}

    async def discover_jobs_by_category(
        self,
        category: TossJobCategory,
        page: Page,
        max_jobs: int = 10
    ) -> list[JobListItem]:
        """ì±„ìš© ëª©ë¡ í˜ì´ì§€ì—ì„œ íŠ¹ì • ì§êµ°ì˜ job_idë“¤ì„ ë™ì ìœ¼ë¡œ íƒìƒ‰

        Args:
            category: ì§êµ° ì¹´í…Œê³ ë¦¬
            page: Playwright Page ê°ì²´
            max_jobs: ìµœëŒ€ ìˆ˜ì§‘í•  ê³µê³  ìˆ˜

        Returns:
            list[JobListItem]: ë°œê²¬ëœ ì±„ìš© ê³µê³  ëª©ë¡
        """
        cache_key = category.value
        if cache_key in self._job_list_cache:
            return self._job_list_cache[cache_key][:max_jobs]

        logger.info(f"ğŸ” í† ìŠ¤ {category.value} ì§êµ° ì±„ìš©ê³µê³  íƒìƒ‰ ì¤‘...")

        # ì±„ìš© ëª©ë¡ í˜ì´ì§€ë¡œ ì´ë™
        await page.goto(self.BASE_URL)
        await page.wait_for_timeout(3000)

        # ëª¨ë“  ì±„ìš©ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        all_jobs = await self._fetch_all_job_listings(page)
        logger.info(f"ğŸ“‹ ì´ {len(all_jobs)}ê°œì˜ ì±„ìš©ê³µê³  ë°œê²¬")

        # ì§êµ°ë³„ë¡œ í•„í„°ë§
        keywords = self.CATEGORY_KEYWORDS.get(category, [])
        matched_jobs = []

        for job in all_jobs:
            search_text = f"{job.title} {job.tags}".lower()

            # ë³‘ì—­íŠ¹ë¡€ ê³µê³ ëŠ” MILITARY ì¹´í…Œê³ ë¦¬ì—ì„œë§Œ ë§¤ì¹­
            is_military = any(kw in search_text for kw in ["ì‚°ì—…ê¸°ëŠ¥ìš”ì›", "ì „ë¬¸ì—°êµ¬ìš”ì›", "ë³‘ì—­íŠ¹ë¡€"])

            if category == TossJobCategory.MILITARY:
                if is_military:
                    matched_jobs.append(job)
            else:
                # ë³‘ì—­íŠ¹ë¡€ê°€ ì•„ë‹Œ ê³µê³ ë§Œ ì¼ë°˜ ì¹´í…Œê³ ë¦¬ì—ì„œ ë§¤ì¹­
                # (ë˜ëŠ” í•´ë‹¹ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œì™€ ë³‘ì—­íŠ¹ë¡€ê°€ ë™ì‹œì— ë§¤ì¹­ë˜ë©´ í¬í•¨)
                if any(kw in search_text for kw in keywords):
                    matched_jobs.append(job)

        logger.info(f"âœ… {category.value} ì§êµ°: {len(matched_jobs)}ê°œ ë§¤ì¹­")

        # ìºì‹œ ì €ì¥
        self._job_list_cache[cache_key] = matched_jobs

        return matched_jobs[:max_jobs]

    async def _fetch_all_job_listings(self, page: Page) -> list[JobListItem]:
        """ì±„ìš© ëª©ë¡ í˜ì´ì§€ì—ì„œ ëª¨ë“  ê³µê³  ì •ë³´ë¥¼ ê°€ì ¸ì˜´"""

        # ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ê³µê³  ë¡œë“œ (lazy loading ëŒ€ì‘)
        await self._scroll_to_load_all(page)

        # JavaScriptë¡œ ëª¨ë“  ì±„ìš©ê³µê³  ì •ë³´ ì¶”ì¶œ
        jobs_data = await page.evaluate("""
            () => {
                // ì•Œë ¤ì§„ ê³„ì—´ì‚¬ ëª©ë¡
                const knownCompanies = [
                    'í† ìŠ¤', 'ë±…í¬', 'ì¦ê¶Œ', 'í˜ì´ë¨¼ì¸ ', 'í”Œë ˆì´ìŠ¤', 'ì¸ìŠˆì–´ëŸ°ìŠ¤',
                    'ì”¨ì—‘ìŠ¤', 'ì¸ì»´', 'ì¸ì‚¬ì´íŠ¸', 'ëª¨ë°”ì¼'
                ];

                const links = Array.from(document.querySelectorAll('a[href*="job-detail?job_id="]'));
                return links.map(link => {
                    const url = link.getAttribute('href');
                    const jobIdMatch = url.match(/job_id=(\\d+)/);
                    const jobId = jobIdMatch ? jobIdMatch[1] : '';

                    // ì œëª© ì¶”ì¶œ
                    const titleEl = link.querySelector('p');
                    const title = titleEl ? titleEl.textContent.trim() : '';

                    // íƒœê·¸ ì¶”ì¶œ (ì œëª© ì•„ë˜ì˜ í…ìŠ¤íŠ¸)
                    const listItem = link.querySelector('li') || link;
                    const allText = listItem.textContent || '';
                    const tags = allText.replace(title, '').trim();

                    // ê³„ì—´ì‚¬ ëª©ë¡ ì¶”ì¶œ - ë§ˆì§€ë§‰ divë“¤ì—ì„œ ì•Œë ¤ì§„ ê³„ì—´ì‚¬ëª…ë§Œ í•„í„°ë§
                    const companyDivs = link.querySelectorAll('div > div:last-child > div');
                    const companies = Array.from(companyDivs)
                        .map(d => d.textContent.trim())
                        .filter(t => t && knownCompanies.some(c => t.includes(c)) && !t.includes('ì™¸'));

                    return { jobId, title, tags, companies };
                }).filter(job => job.jobId && job.title);
            }
        """)

        return [
            JobListItem(
                job_id=j["jobId"],
                title=j["title"],
                tags=j["tags"],
                companies=j["companies"]
            )
            for j in jobs_data
        ]

    async def _scroll_to_load_all(self, page: Page, max_scrolls: int = 10):
        """í˜ì´ì§€ë¥¼ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ"""
        prev_count = 0

        for _ in range(max_scrolls):
            # í˜„ì¬ ê³µê³  ìˆ˜ í™•ì¸
            count = await page.evaluate("""
                () => document.querySelectorAll('a[href*="job-detail?job_id="]').length
            """)

            if count == prev_count:
                break  # ë” ì´ìƒ ìƒˆë¡œìš´ ì½˜í…ì¸ ê°€ ì—†ìŒ

            prev_count = count

            # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(1000)

    async def scrape_positions_by_category(
        self,
        category: TossJobCategory,
        headless: bool = True,
        max_jobs: int = 5
    ) -> ScrapedData:
        """íŠ¹ì • ì§êµ°ì˜ í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ (ë™ì  íƒìƒ‰)

        Args:
            category: ì§êµ° ì¹´í…Œê³ ë¦¬
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
            max_jobs: ìµœëŒ€ ìŠ¤í¬ë˜í•‘í•  ê³µê³  ìˆ˜

        Returns:
            ScrapedData: ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°
        """
        logger.info(f"ğŸš€ í† ìŠ¤ {category.value} í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

        positions = []

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=headless)
            page = await browser.new_page()

            # 1. ë™ì ìœ¼ë¡œ job_id íƒìƒ‰
            job_list = await self.discover_jobs_by_category(category, page, max_jobs)

            if not job_list:
                logger.warning(f"âš ï¸ {category.value} ì§êµ°ì˜ ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                await browser.close()
                return ScrapedData(positions=[], source_url=self.BASE_URL)

            logger.info(f"ğŸ“‹ {len(job_list)}ê°œ ê³µê³  ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

            # 2. ê° ê³µê³  ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
            for job_item in job_list:
                try:
                    position = await self._scrape_position(page, job_item.job_id, category)
                    if position:
                        positions.append(position)
                        logger.info(f"âœ… {position.title} ({position.company}) ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
                    await asyncio.sleep(1)  # Rate limiting
                except Exception as e:
                    logger.error(f"âŒ job_id={job_item.job_id} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")

            await browser.close()

        scraped_data = ScrapedData(
            positions=positions,
            scraped_at=datetime.now(),
            source_url=f"{self.BASE_URL}?category={category.value}",
        )

        logger.info(f"âœ… ì´ {len(positions)}ê°œ {category.value} í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        return scraped_data

    async def scrape_all_server_positions(self, headless: bool = True) -> ScrapedData:
        """ëª¨ë“  Server í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ (ë ˆê±°ì‹œ í˜¸í™˜)

        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€

        Returns:
            ScrapedData: ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°
        """
        return await self.scrape_positions_by_category(TossJobCategory.BACKEND, headless)

    def get_available_categories(self) -> list[TossJobCategory]:
        """ìŠ¤í¬ë˜í•‘ ê°€ëŠ¥í•œ ì§êµ° ëª©ë¡ ë°˜í™˜"""
        return list(self.CATEGORY_KEYWORDS.keys())

    def get_job_url(self, job_id: str, company: Optional[str] = None) -> str:
        """job_idë¡œ ì±„ìš©ê³µê³  URL ìƒì„±

        Args:
            job_id: ì±„ìš©ê³µê³  ID
            company: ê³„ì—´ì‚¬ëª… (ì§€ì •ì‹œ ìƒì„¸ í˜ì´ì§€ë¡œ ë°”ë¡œ ì´ë™)

        Returns:
            ì±„ìš©ê³µê³  URL
        """
        from urllib.parse import quote
        base_url = f"{self.JOB_DETAIL_URL}?job_id={job_id}"
        if company:
            # sub_position_idì™€ company íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ë©´ ìƒì„¸ í˜ì´ì§€ë¡œ ë°”ë¡œ ì´ë™
            return f"{base_url}&sub_position_id={job_id}&company={quote(company)}"
        return base_url

    def get_first_job_url_for_category(self, category: TossJobCategory) -> Optional[str]:
        """ì§êµ°ì˜ ì²« ë²ˆì§¸ ì±„ìš©ê³µê³  URL ë°˜í™˜ (ìºì‹œëœ ë°ì´í„°ì—ì„œ)

        Note: ë™ì  ìŠ¤í¬ë˜í•‘ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ë˜ì–´, ìºì‹œëœ ë°ì´í„°ê°€ ìˆì–´ì•¼ URLì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        ìºì‹œê°€ ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        cache_key = category.value
        if cache_key in self._job_list_cache and self._job_list_cache[cache_key]:
            job = self._job_list_cache[cache_key][0]
            # ì²« ë²ˆì§¸ ê³„ì—´ì‚¬ ì •ë³´ê°€ ìˆìœ¼ë©´ ìƒì„¸ í˜ì´ì§€ URL ë°˜í™˜
            company = job.companies[0] if job.companies else None
            return self.get_job_url(job.job_id, company)
        return None

    async def _scrape_position(
        self,
        page: Page,
        job_id: str,
        category: Optional[TossJobCategory] = None
    ) -> Optional[JobRequirement]:
        """ê°œë³„ í¬ì§€ì…˜ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘

        Args:
            page: Playwright Page ê°ì²´
            job_id: ì±„ìš©ê³µê³  ID
            category: ì§êµ° ì¹´í…Œê³ ë¦¬ (optional)

        Returns:
            JobRequirement ë˜ëŠ” None
        """
        url = f"{self.JOB_DETAIL_URL}?job_id={job_id}"
        logger.debug(f"ğŸ“„ ìŠ¤í¬ë˜í•‘: {url}")

        await page.goto(url)
        await page.wait_for_timeout(3000)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        # "ê³µê³  ë³´ê¸°" ë²„íŠ¼ì´ ìˆìœ¼ë©´ í´ë¦­ (ì—¬ëŸ¬ ê³„ì—´ì‚¬ê°€ ë¬¶ì¸ ê²½ìš°)
        detail_url = url  # ê¸°ë³¸ê°’ì€ ì›ë˜ URL
        button_clicked = False
        try:
            clicked = await page.evaluate("""
                () => {
                    const buttons = document.querySelectorAll('button');
                    for (const btn of buttons) {
                        if (btn.textContent.includes('ê³µê³  ë³´ê¸°')) {
                            btn.click();
                            return true;
                        }
                    }
                    return false;
                }
            """)
            if clicked:
                button_clicked = True
                await page.wait_for_timeout(2000)
                # í´ë¦­ í›„ ë³€ê²½ëœ URL ì €ì¥ (sub_position_id, company íŒŒë¼ë¯¸í„° í¬í•¨)
                detail_url = page.url
                logger.debug(f"ğŸ“Œ ìƒì„¸ URL: {detail_url}")
        except Exception:
            pass  # ë²„íŠ¼ì´ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ

        # í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        data = await page.evaluate("""
            () => {
                const result = {
                    title: document.querySelector('h1')?.textContent?.trim() || '',
                    company: '',
                    requirements: [],
                    preferred: [],
                    tech_stack: [],
                    responsibilities: [],
                };

                // íšŒì‚¬ ì •ë³´ ì¶”ì¶œ (h5 íƒœê·¸ ë˜ëŠ” "ì†Œì†" í…ìŠ¤íŠ¸ ê·¼ì²˜)
                const h5 = document.querySelector('h5');
                if (h5) {
                    const h5Text = h5.textContent?.trim() || '';
                    // "í† ìŠ¤ ì†Œì†" í˜•íƒœì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ
                    result.company = h5Text.replace('ì†Œì†', '').trim() || h5Text;
                }

                // ì„¹ì…˜ë³„ ë°ì´í„° ì¶”ì¶œ (p + ul êµ¬ì¡°)
                const paragraphs = document.querySelectorAll('p');

                for (const p of paragraphs) {
                    const text = p.textContent?.trim() || '';
                    let sibling = p.nextElementSibling;

                    // ì¸ì¬ìƒ / ìê²©ìš”ê±´
                    if (text.includes('ì´ëŸ° ë¶„ê³¼ í•¨ê»˜í•˜ê³  ì‹¶ì–´ìš”') ||
                        text.includes('ì´ëŸ° ë¶„ì„ ì°¾ê³  ìˆì–´ìš”') ||
                        text.includes('ì´ëŸ° ë¶„ì„ ê¸°ë‹¤ë¦¬ê³  ìˆì–´ìš”') ||
                        text.includes('ìê²©ìš”ê±´')) {
                        while (sibling && sibling.tagName === 'UL') {
                            const items = sibling.querySelectorAll('li');
                            items.forEach(item => {
                                const itemText = item.textContent?.trim()?.replace(/^â€¢\\s*/, '');
                                if (itemText) result.requirements.push(itemText);
                            });
                            sibling = sibling.nextElementSibling;
                        }
                    }

                    // ìš°ëŒ€ì‚¬í•­
                    if (text.includes('ì´ëŸ° ë¶„ì´ë©´ ë” ì¢‹ì•„ìš”') || text.includes('ìš°ëŒ€')) {
                        while (sibling && sibling.tagName === 'UL') {
                            const items = sibling.querySelectorAll('li');
                            items.forEach(item => {
                                const itemText = item.textContent?.trim()?.replace(/^â€¢\\s*/, '');
                                if (itemText) result.preferred.push(itemText);
                            });
                            sibling = sibling.nextElementSibling;
                        }
                    }

                    // ê¸°ìˆ  ìŠ¤íƒ
                    if (text.includes('ì‚¬ìš©í•˜ëŠ” ê¸°ìˆ ') || text.includes('ê¸°ìˆ  ìŠ¤íƒ') || text.includes('ê¸°ìˆ ì„')) {
                        while (sibling && sibling.tagName === 'UL') {
                            const items = sibling.querySelectorAll('li');
                            items.forEach(item => {
                                const itemText = item.textContent?.trim()?.replace(/^â€¢\\s*/, '');
                                if (itemText) result.tech_stack.push(itemText);
                            });
                            sibling = sibling.nextElementSibling;
                        }
                    }

                    // ì£¼ìš” ì—…ë¬´
                    if (text.includes('í•©ë¥˜í•˜ë©´ í•¨ê»˜') || text.includes('ì£¼ìš” ì—…ë¬´') || text.includes('ì—…ë¬´ì˜ˆìš”')) {
                        while (sibling && sibling.tagName === 'UL') {
                            const items = sibling.querySelectorAll('li');
                            items.forEach(item => {
                                const itemText = item.textContent?.trim()?.replace(/^â€¢\\s*/, '');
                                if (itemText) result.responsibilities.push(itemText);
                            });
                            sibling = sibling.nextElementSibling;
                        }
                    }
                }

                return result;
            }
        """)

        if not data.get("title") or not data.get("requirements"):
            logger.warning(f"âš ï¸ job_id={job_id}: í•„ìˆ˜ ë°ì´í„° ëˆ„ë½ (ì´ë¯¸ì§€ ì „ìš© ê³µê³ ì¼ ìˆ˜ ìˆìŒ)")
            return None

        # TossJobCategory -> PositionCategory ë§¤í•‘
        from .models import TOSS_TO_POSITION_MAPPING
        position_category = TOSS_TO_POSITION_MAPPING.get(
            category, PositionCategory.BACKEND
        ) if category else PositionCategory.BACKEND

        # ë²„íŠ¼ í´ë¦­ ì—†ì´ ë‹¨ì¼ ê³„ì—´ì‚¬ ê³µê³ ì¸ ê²½ìš°, íšŒì‚¬ ì •ë³´ë¡œ ìƒì„¸ URL êµ¬ì„±
        company = data.get("company", "í† ìŠ¤")
        if not button_clicked and company:
            from urllib.parse import quote
            detail_url = f"{self.JOB_DETAIL_URL}?job_id={job_id}&sub_position_id={job_id}&company={quote(company)}"

        return JobRequirement(
            title=data["title"],
            company=company,
            requirements=data.get("requirements", []),
            preferred=data.get("preferred", []),
            tech_stack=data.get("tech_stack", []),
            responsibilities=data.get("responsibilities", []),
            job_id=job_id,
            detail_url=detail_url,
            category=position_category,
            scraped_at=datetime.now(),
        )

    def save_scraped_data(self, data: ScrapedData) -> None:
        """ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì €ì¥

        Args:
            data: ì €ì¥í•  ScrapedData
        """
        with open(self.scraped_data_path, "w", encoding="utf-8") as f:
            json.dump(data.to_dict(), f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.scraped_data_path}")

    def load_scraped_data(self) -> Optional[ScrapedData]:
        """ì €ì¥ëœ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ

        Returns:
            ScrapedData ë˜ëŠ” None
        """
        if not self.scraped_data_path.exists():
            return None

        try:
            with open(self.scraped_data_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            return ScrapedData.from_dict(data)
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def get_content_hash(self) -> Optional[str]:
        """ì €ì¥ëœ ë°ì´í„°ì˜ content_hash ë°˜í™˜

        Returns:
            content_hash ë˜ëŠ” None
        """
        data = self.load_scraped_data()
        return data.content_hash if data else None

    def has_changes(self, new_data: ScrapedData) -> bool:
        """ë°ì´í„° ë³€ê²½ ì—¬ë¶€ í™•ì¸

        Args:
            new_data: ìƒˆë¡œ ìŠ¤í¬ë˜í•‘í•œ ë°ì´í„°

        Returns:
            ë³€ê²½ ì—¬ë¶€
        """
        old_hash = self.get_content_hash()
        if old_hash is None:
            return True
        return old_hash != new_data.content_hash


async def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    logging.basicConfig(level=logging.INFO)

    scraper = TossJobScraper()

    # Frontend ì§êµ° ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª Frontend ì§êµ° ë™ì  ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸")
    data = await scraper.scrape_positions_by_category(
        TossJobCategory.FRONTEND,
        headless=True,
        max_jobs=3
    )

    print(f"\nğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼: {len(data.positions)}ê°œ í¬ì§€ì…˜")
    for pos in data.positions:
        print(f"  - {pos.title} ({pos.company})")
        print(f"    ì¸ì¬ìƒ: {len(pos.requirements)}ê°œ í•­ëª©")
        print(f"    ê¸°ìˆ ìŠ¤íƒ: {len(pos.tech_stack)}ê°œ í•­ëª©")

    scraper.save_scraped_data(data)


if __name__ == "__main__":
    asyncio.run(main())
