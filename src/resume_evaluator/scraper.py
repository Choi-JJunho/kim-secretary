"""í† ìŠ¤ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼ (Playwright ê¸°ë°˜)"""

import asyncio
import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Optional

from playwright.async_api import async_playwright, Page, Browser

from .models import JobRequirement, ScrapedData, PositionCategory

logger = logging.getLogger(__name__)


class TossJobScraper:
    """í† ìŠ¤ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼"""

    BASE_URL = "https://toss.im/career/jobs"
    JOB_DETAIL_URL = "https://toss.im/career/job-detail"

    # Server/Backend ê´€ë ¨ job_id ëª©ë¡
    SERVER_JOB_IDS = [
        "4071141003",  # Server Developer
        "6085421003",  # Server Developer [Commerce]
        "6052536003",  # Server Developer [Staff]
        "4773428003",  # Server Developer [ì‚°ì—…ê¸°ëŠ¥ìš”ì›/ì „ë¬¸ì—°êµ¬ìš”ì›]
        "4421106003",  # Node.js Developer
        "4328355003",  # Python Developer
        "5847608003",  # Python Developer [ì‚°ì—…ê¸°ëŠ¥ìš”ì›/ì „ë¬¸ì—°êµ¬ìš”ì›]
        "7519850003",  # Tech Lead (Server)
    ]

    def __init__(self, data_dir: str = "data/resume_evaluator"):
        """
        Args:
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.scraped_data_path = self.data_dir / "scraped_positions.json"

    async def scrape_all_server_positions(self, headless: bool = True) -> ScrapedData:
        """ëª¨ë“  Server í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘

        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€

        Returns:
            ScrapedData: ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°
        """
        logger.info("ğŸš€ í† ìŠ¤ Server í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

        positions = []

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=headless)
            page = await browser.new_page()

            for job_id in self.SERVER_JOB_IDS:
                try:
                    position = await self._scrape_position(page, job_id)
                    if position:
                        positions.append(position)
                        logger.info(f"âœ… {position.title} ({position.company}) ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
                    await asyncio.sleep(1)  # Rate limiting
                except Exception as e:
                    logger.error(f"âŒ job_id={job_id} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")

            await browser.close()

        scraped_data = ScrapedData(
            positions=positions,
            scraped_at=datetime.now(),
            source_url=self.BASE_URL,
        )

        logger.info(f"âœ… ì´ {len(positions)}ê°œ í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        return scraped_data

    async def _scrape_position(self, page: Page, job_id: str) -> Optional[JobRequirement]:
        """ê°œë³„ í¬ì§€ì…˜ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘

        Args:
            page: Playwright Page ê°ì²´
            job_id: ì±„ìš©ê³µê³  ID

        Returns:
            JobRequirement ë˜ëŠ” None
        """
        url = f"{self.JOB_DETAIL_URL}?job_id={job_id}"
        logger.debug(f"ğŸ“„ ìŠ¤í¬ë˜í•‘: {url}")

        await page.goto(url)
        await page.wait_for_timeout(3000)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

        # "ê³µê³  ë³´ê¸°" ë²„íŠ¼ì´ ìˆìœ¼ë©´ í´ë¦­ (ì—¬ëŸ¬ ê³„ì—´ì‚¬ê°€ ë¬¶ì¸ ê²½ìš°)
        try:
            view_button = page.locator('button:has-text("ê³µê³  ë³´ê¸°")').first
            if await view_button.is_visible():
                await page.evaluate("""
                    () => {
                        const buttons = document.querySelectorAll('button');
                        for (const btn of buttons) {
                            if (btn.textContent.includes('ê³µê³  ë³´ê¸°')) {
                                btn.click();
                                break;
                            }
                        }
                    }
                """)
                await page.wait_for_timeout(2000)
        except Exception:
            pass  # ë²„íŠ¼ì´ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ

        # í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        data = await page.evaluate("""
            () => {
                const result = {
                    title: document.querySelector('h1')?.textContent?.trim() || '',
                    company: '',
                    requirements: [],
                    preferred: [],
                    tech_stack: [],
                    responsibilities: [],
                };

                // íšŒì‚¬ ì •ë³´ ì¶”ì¶œ
                const h5 = document.querySelector('h5');
                if (h5) {
                    result.company = h5.textContent?.trim() || '';
                }

                // ì„¹ì…˜ë³„ ë°ì´í„° ì¶”ì¶œ
                const paragraphs = document.querySelectorAll('p');

                for (const p of paragraphs) {
                    const text = p.textContent?.trim() || '';
                    let sibling = p.nextElementSibling;

                    // ì¸ì¬ìƒ / ìê²©ìš”ê±´
                    if (text.includes('ì´ëŸ° ë¶„ê³¼ í•¨ê»˜í•˜ê³  ì‹¶ì–´ìš”') || text.includes('ì´ëŸ° ë¶„ì„ ì°¾ê³  ìˆì–´ìš”')) {
                        while (sibling && sibling.tagName === 'UL') {
                            const items = sibling.querySelectorAll('li');
                            items.forEach(item => {
                                const itemText = item.textContent?.trim()?.replace(/^â€¢\\s*/, '');
                                if (itemText) result.requirements.push(itemText);
                            });
                            sibling = sibling.nextElementSibling;
                        }
                    }

                    // ìš°ëŒ€ì‚¬í•­
                    if (text.includes('ì´ëŸ° ë¶„ì´ë©´ ë” ì¢‹ì•„ìš”') || text.includes('ìš°ëŒ€')) {
                        while (sibling && sibling.tagName === 'UL') {
                            const items = sibling.querySelectorAll('li');
                            items.forEach(item => {
                                const itemText = item.textContent?.trim()?.replace(/^â€¢\\s*/, '');
                                if (itemText) result.preferred.push(itemText);
                            });
                            sibling = sibling.nextElementSibling;
                        }
                    }

                    // ê¸°ìˆ  ìŠ¤íƒ
                    if (text.includes('ì‚¬ìš©í•˜ëŠ” ê¸°ìˆ ') || text.includes('ê¸°ìˆ  ìŠ¤íƒ')) {
                        while (sibling && sibling.tagName === 'UL') {
                            const items = sibling.querySelectorAll('li');
                            items.forEach(item => {
                                const itemText = item.textContent?.trim()?.replace(/^â€¢\\s*/, '');
                                if (itemText) result.tech_stack.push(itemText);
                            });
                            sibling = sibling.nextElementSibling;
                        }
                    }

                    // ì£¼ìš” ì—…ë¬´
                    if (text.includes('í•©ë¥˜í•˜ë©´ í•¨ê»˜') || text.includes('ì£¼ìš” ì—…ë¬´')) {
                        while (sibling && sibling.tagName === 'UL') {
                            const items = sibling.querySelectorAll('li');
                            items.forEach(item => {
                                const itemText = item.textContent?.trim()?.replace(/^â€¢\\s*/, '');
                                if (itemText) result.responsibilities.push(itemText);
                            });
                            sibling = sibling.nextElementSibling;
                        }
                    }
                }

                return result;
            }
        """)

        if not data.get("title") or not data.get("requirements"):
            logger.warning(f"âš ï¸ job_id={job_id}: í•„ìˆ˜ ë°ì´í„° ëˆ„ë½")
            return None

        return JobRequirement(
            title=data["title"],
            company=data.get("company", "í† ìŠ¤"),
            requirements=data.get("requirements", []),
            preferred=data.get("preferred", []),
            tech_stack=data.get("tech_stack", []),
            responsibilities=data.get("responsibilities", []),
            job_id=job_id,
            category=PositionCategory.BACKEND,
            scraped_at=datetime.now(),
        )

    def save_scraped_data(self, data: ScrapedData) -> None:
        """ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì €ì¥

        Args:
            data: ì €ì¥í•  ScrapedData
        """
        with open(self.scraped_data_path, "w", encoding="utf-8") as f:
            json.dump(data.to_dict(), f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.scraped_data_path}")

    def load_scraped_data(self) -> Optional[ScrapedData]:
        """ì €ì¥ëœ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ

        Returns:
            ScrapedData ë˜ëŠ” None
        """
        if not self.scraped_data_path.exists():
            return None

        try:
            with open(self.scraped_data_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            return ScrapedData.from_dict(data)
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def get_content_hash(self) -> Optional[str]:
        """ì €ì¥ëœ ë°ì´í„°ì˜ content_hash ë°˜í™˜

        Returns:
            content_hash ë˜ëŠ” None
        """
        data = self.load_scraped_data()
        return data.content_hash if data else None

    def has_changes(self, new_data: ScrapedData) -> bool:
        """ë°ì´í„° ë³€ê²½ ì—¬ë¶€ í™•ì¸

        Args:
            new_data: ìƒˆë¡œ ìŠ¤í¬ë˜í•‘í•œ ë°ì´í„°

        Returns:
            ë³€ê²½ ì—¬ë¶€
        """
        old_hash = self.get_content_hash()
        if old_hash is None:
            return True
        return old_hash != new_data.content_hash


async def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    logging.basicConfig(level=logging.INFO)

    scraper = TossJobScraper()
    data = await scraper.scrape_all_server_positions(headless=True)

    print(f"\nğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼: {len(data.positions)}ê°œ í¬ì§€ì…˜")
    for pos in data.positions:
        print(f"  - {pos.title} ({pos.company})")
        print(f"    ì¸ì¬ìƒ: {len(pos.requirements)}ê°œ í•­ëª©")

    scraper.save_scraped_data(data)


if __name__ == "__main__":
    asyncio.run(main())
