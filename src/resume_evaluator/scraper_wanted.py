"""ì›í‹°ë“œ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼ (Playwright ê¸°ë°˜)

ì›í‹°ë“œëŠ” ì—¬ëŸ¬ ê¸°ì—…ì˜ ì±„ìš©ê³µê³ ë¥¼ ëª¨ì•„ë†“ì€ í”Œë«í¼ì…ë‹ˆë‹¤.
ê¸°ì—…ë³„/ì§êµ°ë³„ë¡œ ì±„ìš©ê³µê³ ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ì´ë ¥ì„œ í‰ê°€ì— í™œìš©í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import logging
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Optional
from urllib.parse import quote

from playwright.async_api import async_playwright, Page

from .models import (
    JobRequirement,
    ScrapedData,
    PositionCategory,
    WantedJobCategory,
    WANTED_DUTY_ID_MAP,
    WANTED_TO_POSITION_MAPPING,
)

logger = logging.getLogger(__name__)


@dataclass
class WantedJobListItem:
    """ì›í‹°ë“œ ì±„ìš© ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ê³µê³  ì •ë³´"""
    job_id: str
    title: str
    company: str
    location: str
    experience: str  # ì˜ˆ: "ì‹ ì…-ê²½ë ¥ 4ë…„", "ê²½ë ¥ 3ë…„ ì´ìƒ"


@dataclass
class WantedCompanyInfo:
    """ì›í‹°ë“œì—ì„œ ìŠ¤í¬ë˜í•‘í•œ ê¸°ì—… ì •ë³´"""
    company_id: str
    company_name: str
    industry: str = ""
    positions_count: int = 0


class WantedJobScraper:
    """ì›í‹°ë“œ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼

    ì›í‹°ë“œ í”Œë«í¼ì—ì„œ ì±„ìš©ê³µê³ ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    - ì§êµ°ë³„ í•„í„°ë§ (Backend, Frontend, DevOps ë“±)
    - ê²½ë ¥ í•„í„°ë§ (ì‹ ì…~3ë…„, 3ë…„ ì´ìƒ ë“±)
    - ì§€ì—­ í•„í„°ë§ (ì„œìš¸, íŒêµ ë“±)
    - ê¸°ì—…ë³„ ì±„ìš©ê³µê³  ìˆ˜ì§‘
    """

    BASE_URL = "https://www.wanted.co.kr"
    JOB_LIST_URL = "https://www.wanted.co.kr/wdlist/518"  # ê°œë°œ ì§êµ° ê¸°ë³¸
    JOB_DETAIL_URL = "https://www.wanted.co.kr/wd"

    def __init__(self, data_dir: str = "data/resume_evaluator/wanted"):
        """
        Args:
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.scraped_data_path = self.data_dir / "scraped_positions.json"
        self._job_list_cache: dict[str, list[WantedJobListItem]] = {}

    def _build_list_url(
        self,
        categories: list[WantedJobCategory] | None = None,
        years_min: int = 0,
        years_max: int = 10,
        locations: list[str] | None = None,
    ) -> str:
        """ì±„ìš©ê³µê³  ëª©ë¡ URL ìƒì„±

        Args:
            categories: ì§êµ° ì¹´í…Œê³ ë¦¬ ëª©ë¡
            years_min: ìµœì†Œ ê²½ë ¥ (0=ì‹ ì…)
            years_max: ìµœëŒ€ ê²½ë ¥
            locations: ì§€ì—­ ëª©ë¡ (ì˜ˆ: ["seoul.all", "gyeonggi.bundang"])

        Returns:
            ì™„ì„±ëœ URL
        """
        url = f"{self.JOB_LIST_URL}?"

        # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
        url += "country=kr&job_sort=job.latest_order"

        # ê²½ë ¥ í•„í„°
        url += f"&years={years_min}&years={years_max}"

        # ì§€ì—­ í•„í„°
        if locations:
            for loc in locations:
                url += f"&locations={loc}"
        else:
            url += "&locations=seoul.all"

        # ì§êµ° í•„í„° (selected íŒŒë¼ë¯¸í„°)
        if categories:
            for cat in categories:
                duty_id = WANTED_DUTY_ID_MAP.get(cat)
                if duty_id:
                    url += f"&selected={duty_id}"

        return url

    async def scrape_job_list(
        self,
        page: Page,
        categories: list[WantedJobCategory] | None = None,
        years_min: int = 0,
        years_max: int = 3,
        locations: list[str] | None = None,
        max_jobs: int = 20,
    ) -> list[WantedJobListItem]:
        """ì±„ìš©ê³µê³  ëª©ë¡ ìŠ¤í¬ë˜í•‘

        Args:
            page: Playwright Page ê°ì²´
            categories: ì§êµ° ì¹´í…Œê³ ë¦¬ ëª©ë¡
            years_min: ìµœì†Œ ê²½ë ¥
            years_max: ìµœëŒ€ ê²½ë ¥
            locations: ì§€ì—­ ëª©ë¡
            max_jobs: ìµœëŒ€ ìˆ˜ì§‘í•  ê³µê³  ìˆ˜

        Returns:
            ì±„ìš©ê³µê³  ëª©ë¡
        """
        url = self._build_list_url(categories, years_min, years_max, locations)
        logger.info(f"ğŸ” ì›í‹°ë“œ ì±„ìš©ê³µê³  ëª©ë¡ ìŠ¤í¬ë˜í•‘: {url}")

        await page.goto(url)
        await page.wait_for_timeout(3000)

        # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ê³µê³  ë¡œë“œ
        await self._scroll_to_load_jobs(page, max_jobs)

        # ê³µê³  ëª©ë¡ ì¶”ì¶œ
        jobs_data = await page.evaluate("""
            (maxJobs) => {
                const jobs = [];
                // ê³µê³  ì¹´ë“œ ì„ íƒ - listitem ë‚´ë¶€ì˜ link
                const jobCards = document.querySelectorAll('ul > li > a[href^="/wd/"]');

                for (const card of jobCards) {
                    if (jobs.length >= maxJobs) break;

                    const href = card.getAttribute('href') || '';
                    const jobIdMatch = href.match(/\\/wd\\/(\\d+)/);
                    if (!jobIdMatch) continue;

                    const jobId = jobIdMatch[1];

                    // í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ (êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                    const textContent = card.textContent || '';
                    const divs = card.querySelectorAll('div');

                    let title = '';
                    let company = '';
                    let locationExp = '';

                    // div êµ¬ì¡°ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    for (const div of divs) {
                        const text = div.textContent?.trim() || '';
                        // ì œëª©ì€ ë³´í†µ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸
                        if (text.length > title.length && !text.includes('í•©ê²©ë³´ìƒê¸ˆ') && !text.includes('Â·')) {
                            // ì´ë¯¸ íšŒì‚¬ëª…ì´ ì„¤ì •ëœ ê²½ìš°, ìƒˆ í…ìŠ¤íŠ¸ê°€ ì œëª©ì¼ ê°€ëŠ¥ì„±
                            if (company && text !== company) {
                                title = text;
                            } else if (!company) {
                                title = text;
                            }
                        }
                        // íšŒì‚¬ëª… (ì œëª© ë‹¤ìŒì— ì˜¤ëŠ” ì§§ì€ í…ìŠ¤íŠ¸)
                        if (text.length > 0 && text.length < 50 && !text.includes('í•©ê²©ë³´ìƒê¸ˆ') &&
                            !text.includes('Â·') && text !== title) {
                            company = text;
                        }
                        // ìœ„ì¹˜Â·ê²½ë ¥ ì •ë³´
                        if (text.includes('Â·') && (text.includes('ì„œìš¸') || text.includes('ê²½ë ¥') || text.includes('ì‹ ì…'))) {
                            locationExp = text;
                        }
                    }

                    // ìœ„ì¹˜ì™€ ê²½ë ¥ ë¶„ë¦¬
                    const parts = locationExp.split('Â·').map(s => s.trim());
                    const location = parts[0] || '';
                    const experience = parts[1] || '';

                    if (jobId && title) {
                        jobs.push({
                            jobId,
                            title,
                            company,
                            location,
                            experience
                        });
                    }
                }

                return jobs;
            }
        """, max_jobs)

        result = [
            WantedJobListItem(
                job_id=j["jobId"],
                title=j["title"],
                company=j["company"],
                location=j["location"],
                experience=j["experience"]
            )
            for j in jobs_data
        ]

        logger.info(f"ğŸ“‹ {len(result)}ê°œ ì±„ìš©ê³µê³  ë°œê²¬")
        return result

    async def _scroll_to_load_jobs(self, page: Page, target_count: int, max_scrolls: int = 10):
        """í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ê³µê³  ë¡œë“œ"""
        prev_count = 0

        for _ in range(max_scrolls):
            count = await page.evaluate("""
                () => document.querySelectorAll('ul > li > a[href^="/wd/"]').length
            """)

            if count >= target_count or count == prev_count:
                break

            prev_count = count
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(1500)

    async def scrape_job_detail(
        self,
        page: Page,
        job_id: str,
        category: WantedJobCategory | None = None,
    ) -> Optional[JobRequirement]:
        """ê°œë³„ ì±„ìš©ê³µê³  ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘

        Args:
            page: Playwright Page ê°ì²´
            job_id: ì±„ìš©ê³µê³  ID
            category: ì§êµ° ì¹´í…Œê³ ë¦¬ (optional)

        Returns:
            JobRequirement ë˜ëŠ” None
        """
        url = f"{self.JOB_DETAIL_URL}/{job_id}"
        logger.debug(f"ğŸ“„ ìŠ¤í¬ë˜í•‘: {url}")

        await page.goto(url)
        await page.wait_for_timeout(2000)

        # "ìƒì„¸ ì •ë³´ ë” ë³´ê¸°" ë²„íŠ¼ í´ë¦­ (ìˆìœ¼ë©´)
        try:
            clicked = await page.evaluate("""
                () => {
                    const buttons = document.querySelectorAll('button');
                    for (const btn of buttons) {
                        if (btn.textContent.includes('ìƒì„¸ ì •ë³´ ë” ë³´ê¸°')) {
                            btn.click();
                            return true;
                        }
                    }
                    return false;
                }
            """)
            if clicked:
                await page.wait_for_timeout(1000)
        except Exception:
            pass

        # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        data = await page.evaluate("""
            () => {
                const result = {
                    title: '',
                    company: '',
                    requirements: [],
                    preferred: [],
                    tech_stack: [],
                    responsibilities: [],
                    location: '',
                    deadline: '',
                };

                // ì œëª© (h1 íƒœê·¸)
                const h1 = document.querySelector('h1');
                if (h1) {
                    result.title = h1.textContent?.trim() || '';
                }

                // íšŒì‚¬ëª… (ë§í¬ì—ì„œ ì¶”ì¶œ)
                const companyLink = document.querySelector('a[href^="/company/"]');
                if (companyLink) {
                    result.company = companyLink.textContent?.trim() || '';
                }

                // ì„¹ì…˜ë³„ ì •ë³´ ì¶”ì¶œ (heading + ë‹¤ìŒ ìš”ì†Œ)
                const headings = document.querySelectorAll('h2, h3');

                for (const heading of headings) {
                    const headingText = heading.textContent?.trim().toLowerCase() || '';
                    let nextEl = heading.nextElementSibling;

                    // ì£¼ìš”ì—…ë¬´
                    if (headingText.includes('ì£¼ìš”ì—…ë¬´') || headingText.includes('í¬ì§€ì…˜ ìƒì„¸')) {
                        while (nextEl && !['H2', 'H3'].includes(nextEl.tagName)) {
                            const text = nextEl.textContent?.trim() || '';
                            if (text && !text.includes('ì£¼ìš”ì—…ë¬´') && !text.includes('í•©ë¥˜í•˜ë©´')) {
                                // ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬ëœ í•­ëª©ë“¤ ì²˜ë¦¬
                                const lines = text.split(/[â€¢\\n]/).filter(l => l.trim());
                                for (const line of lines) {
                                    const cleaned = line.trim();
                                    if (cleaned && cleaned.length > 5 && !result.responsibilities.includes(cleaned)) {
                                        result.responsibilities.push(cleaned);
                                    }
                                }
                            }
                            nextEl = nextEl.nextElementSibling;
                        }
                    }

                    // ìê²©ìš”ê±´
                    if (headingText.includes('ìê²©ìš”ê±´') || headingText.includes('ì´ëŸ° ë¶„')) {
                        while (nextEl && !['H2', 'H3'].includes(nextEl.tagName)) {
                            const text = nextEl.textContent?.trim() || '';
                            if (text && !text.includes('ìê²©ìš”ê±´') && !text.includes('ì´ëŸ° ë¶„')) {
                                const lines = text.split(/[â€¢\\n]/).filter(l => l.trim());
                                for (const line of lines) {
                                    const cleaned = line.trim();
                                    if (cleaned && cleaned.length > 5 && !result.requirements.includes(cleaned)) {
                                        result.requirements.push(cleaned);
                                    }
                                }
                            }
                            nextEl = nextEl.nextElementSibling;
                        }
                    }

                    // ìš°ëŒ€ì‚¬í•­
                    if (headingText.includes('ìš°ëŒ€ì‚¬í•­') || headingText.includes('ìš°ëŒ€')) {
                        while (nextEl && !['H2', 'H3'].includes(nextEl.tagName)) {
                            const text = nextEl.textContent?.trim() || '';
                            if (text && !text.includes('ìš°ëŒ€ì‚¬í•­')) {
                                const lines = text.split(/[â€¢\\n]/).filter(l => l.trim());
                                for (const line of lines) {
                                    const cleaned = line.trim();
                                    if (cleaned && cleaned.length > 5 && !result.preferred.includes(cleaned)) {
                                        result.preferred.push(cleaned);
                                    }
                                }
                            }
                            nextEl = nextEl.nextElementSibling;
                        }
                    }

                    // ê¸°ìˆ ìŠ¤íƒ (í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ)
                    if (headingText.includes('ê¸°ìˆ ') || headingText.includes('stack')) {
                        while (nextEl && !['H2', 'H3'].includes(nextEl.tagName)) {
                            const text = nextEl.textContent?.trim() || '';
                            if (text) {
                                const lines = text.split(/[â€¢\\n,]/).filter(l => l.trim());
                                for (const line of lines) {
                                    const cleaned = line.trim();
                                    if (cleaned && cleaned.length > 1 && !result.tech_stack.includes(cleaned)) {
                                        result.tech_stack.push(cleaned);
                                    }
                                }
                            }
                            nextEl = nextEl.nextElementSibling;
                        }
                    }

                    // ë§ˆê°ì¼
                    if (headingText.includes('ë§ˆê°')) {
                        if (nextEl) {
                            result.deadline = nextEl.textContent?.trim() || '';
                        }
                    }

                    // ê·¼ë¬´ì§€ì—­
                    if (headingText.includes('ê·¼ë¬´ì§€ì—­') || headingText.includes('ìœ„ì¹˜')) {
                        if (nextEl) {
                            result.location = nextEl.textContent?.trim() || '';
                        }
                    }
                }

                // ê¸°ìˆ ìŠ¤íƒì´ ë¹„ì–´ìˆìœ¼ë©´ ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                if (result.tech_stack.length === 0) {
                    const bodyText = document.body.textContent || '';
                    const techPatterns = [
                        /Core:\\s*([^\\n]+)/i,
                        /Data.*?Messaging:\\s*([^\\n]+)/i,
                        /DevOps.*?Infra:\\s*([^\\n]+)/i,
                        /ì‚¬ìš©í•˜ëŠ” ê¸°ìˆ [:\\s]*([^\\n]+)/i,
                    ];

                    for (const pattern of techPatterns) {
                        const match = bodyText.match(pattern);
                        if (match && match[1]) {
                            const techs = match[1].split(/[,ã€]/).map(t => t.trim()).filter(t => t);
                            result.tech_stack.push(...techs);
                        }
                    }
                }

                return result;
            }
        """)

        if not data.get("title"):
            logger.warning(f"âš ï¸ job_id={job_id}: ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None

        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        position_category = WANTED_TO_POSITION_MAPPING.get(
            category, PositionCategory.OTHER
        ) if category else PositionCategory.OTHER

        return JobRequirement(
            title=data["title"],
            company=data.get("company", ""),
            requirements=data.get("requirements", []),
            preferred=data.get("preferred", []),
            tech_stack=data.get("tech_stack", []),
            responsibilities=data.get("responsibilities", []),
            job_id=job_id,
            detail_url=f"{self.JOB_DETAIL_URL}/{job_id}",
            category=position_category,
            scraped_at=datetime.now(),
        )

    async def scrape_positions_by_category(
        self,
        categories: list[WantedJobCategory],
        headless: bool = True,
        max_jobs: int = 10,
        years_min: int = 0,
        years_max: int = 3,
    ) -> ScrapedData:
        """íŠ¹ì • ì§êµ°ë“¤ì˜ í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘

        Args:
            categories: ì§êµ° ì¹´í…Œê³ ë¦¬ ëª©ë¡
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
            max_jobs: ìµœëŒ€ ìŠ¤í¬ë˜í•‘í•  ê³µê³  ìˆ˜
            years_min: ìµœì†Œ ê²½ë ¥
            years_max: ìµœëŒ€ ê²½ë ¥

        Returns:
            ScrapedData
        """
        category_names = ", ".join(c.value for c in categories)
        logger.info(f"ğŸš€ ì›í‹°ë“œ {category_names} í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

        positions = []

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=headless)
            page = await browser.new_page()

            # 1. ê³µê³  ëª©ë¡ ìŠ¤í¬ë˜í•‘
            job_list = await self.scrape_job_list(
                page,
                categories=categories,
                years_min=years_min,
                years_max=years_max,
                max_jobs=max_jobs,
            )

            if not job_list:
                logger.warning(f"âš ï¸ {category_names} ì§êµ°ì˜ ì±„ìš©ê³µê³ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                await browser.close()
                return ScrapedData(positions=[], source_url=self.JOB_LIST_URL)

            logger.info(f"ğŸ“‹ {len(job_list)}ê°œ ê³µê³  ìƒì„¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

            # 2. ê° ê³µê³  ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
            for job_item in job_list:
                try:
                    # ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
                    position = await self.scrape_job_detail(
                        page,
                        job_item.job_id,
                        categories[0] if categories else None
                    )
                    if position:
                        # ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ íšŒì‚¬ëª… ë³´ì™„
                        if not position.company and job_item.company:
                            position.company = job_item.company
                        positions.append(position)
                        logger.info(f"âœ… {position.title} ({position.company}) ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
                    await asyncio.sleep(1)  # Rate limiting
                except Exception as e:
                    logger.error(f"âŒ job_id={job_item.job_id} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")

            await browser.close()

        url = self._build_list_url(categories, years_min, years_max)
        scraped_data = ScrapedData(
            positions=positions,
            scraped_at=datetime.now(),
            source_url=url,
        )

        logger.info(f"âœ… ì´ {len(positions)}ê°œ í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        return scraped_data

    async def scrape_all_dev_positions(
        self,
        headless: bool = True,
        max_jobs: int = 20,
    ) -> ScrapedData:
        """ëª¨ë“  ê°œë°œ ì§êµ° í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ (ì‹ ì…~3ë…„)

        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
            max_jobs: ìµœëŒ€ ìŠ¤í¬ë˜í•‘í•  ê³µê³  ìˆ˜

        Returns:
            ScrapedData
        """
        categories = [
            WantedJobCategory.BACKEND,
            WantedJobCategory.FRONTEND,
            WantedJobCategory.FULLSTACK,
            WantedJobCategory.DEVOPS,
        ]
        return await self.scrape_positions_by_category(
            categories,
            headless=headless,
            max_jobs=max_jobs
        )

    async def scrape_company_positions(
        self,
        company_name: str,
        headless: bool = True,
        max_jobs: int = 10,
    ) -> ScrapedData:
        """íŠ¹ì • ê¸°ì—…ì˜ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í•‘

        Args:
            company_name: ê¸°ì—…ëª…
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
            max_jobs: ìµœëŒ€ ìŠ¤í¬ë˜í•‘í•  ê³µê³  ìˆ˜

        Returns:
            ScrapedData
        """
        logger.info(f"ğŸ¢ {company_name} ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

        # ë¨¼ì € ì „ì²´ ê³µê³ ë¥¼ ê°€ì ¸ì˜¨ í›„ ê¸°ì—…ëª…ìœ¼ë¡œ í•„í„°ë§
        all_data = await self.scrape_all_dev_positions(headless=headless, max_jobs=50)

        # ê¸°ì—…ëª…ìœ¼ë¡œ í•„í„°ë§
        company_positions = [
            p for p in all_data.positions
            if company_name.lower() in p.company.lower()
        ]

        logger.info(f"âœ… {company_name}: {len(company_positions)}ê°œ í¬ì§€ì…˜ ë°œê²¬")

        return ScrapedData(
            positions=company_positions[:max_jobs],
            scraped_at=datetime.now(),
            source_url=f"{self.BASE_URL}/search?query={quote(company_name)}",
        )

    def save_scraped_data(self, data: ScrapedData, filename: str | None = None) -> Path:
        """ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì €ì¥

        Args:
            data: ì €ì¥í•  ScrapedData
            filename: íŒŒì¼ëª… (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        filepath = self.data_dir / (filename or "scraped_positions.json")
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data.to_dict(), f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
        return filepath

    def load_scraped_data(self, filename: str | None = None) -> Optional[ScrapedData]:
        """ì €ì¥ëœ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ

        Args:
            filename: íŒŒì¼ëª… (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)

        Returns:
            ScrapedData ë˜ëŠ” None
        """
        filepath = self.data_dir / (filename or "scraped_positions.json")
        if not filepath.exists():
            return None

        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
            return ScrapedData.from_dict(data)
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def get_available_categories(self) -> list[WantedJobCategory]:
        """ìŠ¤í¬ë˜í•‘ ê°€ëŠ¥í•œ ì§êµ° ëª©ë¡ ë°˜í™˜"""
        return list(WANTED_DUTY_ID_MAP.keys())


async def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    logging.basicConfig(level=logging.INFO)

    scraper = WantedJobScraper()

    # Backend ì§êµ° ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª ì›í‹°ë“œ Backend ì§êµ° ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸")
    data = await scraper.scrape_positions_by_category(
        [WantedJobCategory.BACKEND, WantedJobCategory.JAVA],
        headless=True,
        max_jobs=5
    )

    print(f"\nğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼: {len(data.positions)}ê°œ í¬ì§€ì…˜")
    for pos in data.positions:
        print(f"  - {pos.title} ({pos.company})")
        print(f"    ìê²©ìš”ê±´: {len(pos.requirements)}ê°œ í•­ëª©")
        print(f"    ê¸°ìˆ ìŠ¤íƒ: {pos.tech_stack[:5] if pos.tech_stack else 'ì—†ìŒ'}")

    scraper.save_scraped_data(data)


if __name__ == "__main__":
    asyncio.run(main())
