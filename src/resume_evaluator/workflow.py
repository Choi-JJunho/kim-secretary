"""ì´ë ¥ì„œ í‰ê°€ ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

í”Œë¡œìš°:
1. ì§êµ° ë¶„ë¥˜: ì´ë ¥ì„œ ë¶„ì„í•˜ì—¬ ì í•©í•œ ì§êµ° ì¶”ì²œ
2. ìŠ¤í¬ë˜í•‘: í•´ë‹¹ ì§êµ°ì˜ í† ìŠ¤ ì±„ìš©ê³µê³ ì—ì„œ ì¸ì¬ìƒ ìˆ˜ì§‘
3. í”„ë¡¬í”„íŠ¸ ìƒì„±: ì¸ì¬ìƒ ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
4. í‰ê°€: AI Agentê°€ ì´ë ¥ì„œ í‰ê°€
"""

import json
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional

from .models import ScrapedData, GeneratedPrompt, EvaluationResult, TossJobCategory
from .scraper import TossJobScraper
from .prompt_generator import PromptGenerator
from .evaluator import ResumeEvaluator
from .job_classifier import JobClassifier, ClassificationResult

logger = logging.getLogger(__name__)


@dataclass
class WorkflowConfig:
    """ì›Œí¬í”Œë¡œìš° ì„¤ì •"""
    data_dir: str = "data/resume_evaluator"
    ai_provider: str = "claude"
    target_position: str = "Backend"
    headless: bool = True
    force_scrape: bool = False
    force_regenerate: bool = False
    auto_classify: bool = True  # ì´ë ¥ì„œì—ì„œ ì§êµ° ìë™ ë¶„ë¥˜


@dataclass
class EvaluationResultWithClassification:
    """ì§êµ° ë¶„ë¥˜ ê²°ê³¼ë¥¼ í¬í•¨í•œ í‰ê°€ ê²°ê³¼"""
    classification: ClassificationResult
    evaluation: EvaluationResult
    recommended_job_urls: list[str] = field(default_factory=list)


class ResumeEvaluationWorkflow:
    """ì´ë ¥ì„œ í‰ê°€ ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

    ì›Œí¬í”Œë¡œìš°:
    1. ì§êµ° ë¶„ë¥˜: ì´ë ¥ì„œ ë¶„ì„í•˜ì—¬ ì í•©í•œ ì§êµ° ì¶”ì²œ
    2. ìŠ¤í¬ë˜í•‘: í•´ë‹¹ ì§êµ°ì˜ í† ìŠ¤ ì±„ìš©ê³µê³ ì—ì„œ ì¸ì¬ìƒ ìˆ˜ì§‘
    3. í”„ë¡¬í”„íŠ¸ ìƒì„±: ì¸ì¬ìƒ ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ë³€ê²½ ì‹œì—ë§Œ)
    4. í‰ê°€: AI Agentê°€ ì´ë ¥ì„œ í‰ê°€
    """

    def __init__(self, config: Optional[WorkflowConfig] = None):
        """
        Args:
            config: ì›Œí¬í”Œë¡œìš° ì„¤ì •
        """
        self.config = config or WorkflowConfig()
        self.data_dir = Path(self.config.data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.scraper = TossJobScraper(data_dir=self.config.data_dir)
        self.prompt_generator = PromptGenerator(data_dir=self.config.data_dir)
        self.evaluator = ResumeEvaluator(
            ai_provider=self.config.ai_provider,
            data_dir=self.config.data_dir
        )
        self.classifier = JobClassifier(ai_provider=self.config.ai_provider)

        # ìƒíƒœ
        self._scraped_data: Optional[ScrapedData] = None
        self._generated_prompt: Optional[GeneratedPrompt] = None
        self._classification_result: Optional[ClassificationResult] = None
        self._initialized = False

    async def initialize(self) -> bool:
        """ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” (ìŠ¤í¬ë˜í•‘ + í”„ë¡¬í”„íŠ¸ ìƒì„±)

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        logger.info("ğŸš€ ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì‹œì‘...")

        try:
            # Step 1: ìŠ¤í¬ë˜í•‘
            scraped_data = await self._run_scraping()

            # Step 2: í”„ë¡¬í”„íŠ¸ ìƒì„± (í•„ìš” ì‹œ)
            generated_prompt = self._run_prompt_generation(scraped_data)

            # Step 3: Evaluatorì— í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            self.evaluator.load_system_prompt(generated_prompt)

            self._initialized = True
            logger.info("âœ… ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def _run_scraping(self) -> ScrapedData:
        """ìŠ¤í¬ë˜í•‘ ë‹¨ê³„ ì‹¤í–‰

        Returns:
            ScrapedData: ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°
        """
        logger.info("ğŸ“¡ ìŠ¤í¬ë˜í•‘ ë‹¨ê³„ ì‹œì‘...")

        # ê¸°ì¡´ ë°ì´í„° í™•ì¸
        existing_data = self.scraper.load_scraped_data()

        if existing_data and not self.config.force_scrape:
            logger.info("ğŸ“¦ ê¸°ì¡´ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì‚¬ìš©")
            self._scraped_data = existing_data
            return existing_data

        # ìƒˆë¡œ ìŠ¤í¬ë˜í•‘
        logger.info("ğŸ”„ ìƒˆë¡œìš´ ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰...")
        scraped_data = await self.scraper.scrape_all_server_positions(
            headless=self.config.headless
        )

        # ë³€ê²½ ì—¬ë¶€ í™•ì¸
        if existing_data:
            if self.scraper.has_changes(scraped_data):
                logger.info("ğŸ†• ìŠ¤í¬ë˜í•‘ ë°ì´í„°ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logger.info("âœ… ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë³€ê²½ ì—†ìŒ")

        # ì €ì¥
        self.scraper.save_scraped_data(scraped_data)
        self._scraped_data = scraped_data

        return scraped_data

    def _run_prompt_generation(self, scraped_data: ScrapedData) -> GeneratedPrompt:
        """í”„ë¡¬í”„íŠ¸ ìƒì„± ë‹¨ê³„ ì‹¤í–‰

        Args:
            scraped_data: ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°

        Returns:
            GeneratedPrompt: ìƒì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        logger.info("ğŸ“ í”„ë¡¬í”„íŠ¸ ìƒì„± ë‹¨ê³„ ì‹œì‘...")

        # ì¬ìƒì„± í•„ìš” ì—¬ë¶€ í™•ì¸
        needs_regen = self.prompt_generator.needs_regeneration(scraped_data.content_hash)

        if not needs_regen and not self.config.force_regenerate:
            existing_prompt = self.prompt_generator.load_prompt()
            if existing_prompt:
                logger.info("ğŸ“¦ ê¸°ì¡´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ë°ì´í„° ë³€ê²½ ì—†ìŒ)")
                self._generated_prompt = existing_prompt
                return existing_prompt

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        logger.info("ğŸ”„ ìƒˆë¡œìš´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±...")
        generated_prompt = self.prompt_generator.generate_system_prompt(
            scraped_data=scraped_data,
            target_position=self.config.target_position
        )

        # ì €ì¥
        self.prompt_generator.save_prompt(generated_prompt)
        self._generated_prompt = generated_prompt

        return generated_prompt

    async def evaluate_resume(
        self,
        resume_text: str,
        position: str = "Server Developer"
    ) -> EvaluationResult:
        """ì´ë ¥ì„œ í‰ê°€

        Args:
            resume_text: ì´ë ¥ì„œ í…ìŠ¤íŠ¸
            position: ì§€ì› í¬ì§€ì…˜

        Returns:
            EvaluationResult: í‰ê°€ ê²°ê³¼
        """
        if not self._initialized:
            logger.info("âš ï¸ ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ˆê¸°í™”ë¥¼ ë¨¼ì € ìˆ˜í–‰í•©ë‹ˆë‹¤...")
            await self.initialize()

        return await self.evaluator.evaluate(resume_text, position)

    async def evaluate_resume_file(
        self,
        file_path: str,
        position: str = "Server Developer"
    ) -> EvaluationResult:
        """íŒŒì¼ì—ì„œ ì´ë ¥ì„œë¥¼ ì½ì–´ í‰ê°€

        Args:
            file_path: ì´ë ¥ì„œ íŒŒì¼ ê²½ë¡œ
            position: ì§€ì› í¬ì§€ì…˜

        Returns:
            EvaluationResult: í‰ê°€ ê²°ê³¼
        """
        if not self._initialized:
            logger.info("âš ï¸ ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ˆê¸°í™”ë¥¼ ë¨¼ì € ìˆ˜í–‰í•©ë‹ˆë‹¤...")
            await self.initialize()

        return await self.evaluator.evaluate_from_file(file_path, position)

    async def classify_resume(self, resume_text: str) -> ClassificationResult:
        """ì´ë ¥ì„œ ì§êµ° ë¶„ë¥˜

        Args:
            resume_text: ì´ë ¥ì„œ í…ìŠ¤íŠ¸

        Returns:
            ClassificationResult: ë¶„ë¥˜ ê²°ê³¼
        """
        logger.info("ğŸ” ì´ë ¥ì„œ ì§êµ° ë¶„ë¥˜ ì‹œì‘...")
        result = await self.classifier.classify(resume_text)
        self._classification_result = result
        logger.info(f"âœ… ì§êµ° ë¶„ë¥˜ ì™„ë£Œ: {result.primary_category.value} (ì‹ ë¢°ë„: {result.confidence:.0%})")
        return result

    async def classify_resume_file(self, file_path: str) -> ClassificationResult:
        """íŒŒì¼ì—ì„œ ì´ë ¥ì„œë¥¼ ì½ì–´ ì§êµ° ë¶„ë¥˜

        Args:
            file_path: ì´ë ¥ì„œ íŒŒì¼ ê²½ë¡œ

        Returns:
            ClassificationResult: ë¶„ë¥˜ ê²°ê³¼
        """
        return await self.classifier.classify_from_file(file_path)

    async def evaluate_with_classification(
        self,
        file_path: str
    ) -> EvaluationResultWithClassification:
        """ì§êµ° ë¶„ë¥˜ í›„ í•´ë‹¹ ì§êµ° ê¸°ì¤€ìœ¼ë¡œ í‰ê°€

        ìƒˆë¡œìš´ í”Œë¡œìš°:
        1. ì´ë ¥ì„œì—ì„œ ì§êµ° ë¶„ë¥˜
        2. í•´ë‹¹ ì§êµ°ì˜ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í•‘
        3. í”„ë¡¬í”„íŠ¸ ìƒì„±
        4. ì´ë ¥ì„œ í‰ê°€

        Args:
            file_path: ì´ë ¥ì„œ íŒŒì¼ ê²½ë¡œ

        Returns:
            EvaluationResultWithClassification: ë¶„ë¥˜ + í‰ê°€ ê²°ê³¼
        """
        # Step 1: ì§êµ° ë¶„ë¥˜
        classification = await self.classify_resume_file(file_path)
        primary_category = classification.primary_category
        logger.info(f"ğŸ“Š ë¶„ë¥˜ëœ ì§êµ°: {primary_category.value}")

        # Step 2: í•´ë‹¹ ì§êµ°ì˜ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í•‘ (ìºì‹œëœ ë°ì´í„° ìš°ì„  ì‚¬ìš©)
        scraped_data = await self._run_scraping_for_category(primary_category)

        # Step 3: í”„ë¡¬í”„íŠ¸ ìƒì„±
        if scraped_data.positions:
            generated_prompt = self._run_prompt_generation(scraped_data)
            self.evaluator.load_system_prompt(generated_prompt)
            self._initialized = True
        else:
            # í´ë°±: ê¸°ì¡´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            logger.warning(f"âš ï¸ {primary_category.value} ì§êµ°ì˜ ì±„ìš©ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
            if not self._initialized:
                await self.initialize()

        # Step 4: ì´ë ¥ì„œ í‰ê°€
        position_name = self._get_position_name(primary_category)
        evaluation = await self.evaluator.evaluate_from_file(file_path, position_name)

        # ì¶”ì²œ ì±„ìš©ê³µê³  URL ìƒì„±
        recommended_urls = self._get_recommended_job_urls(primary_category, classification.secondary_categories)

        return EvaluationResultWithClassification(
            classification=classification,
            evaluation=evaluation,
            recommended_job_urls=recommended_urls,
        )

    async def _run_scraping_for_category(self, category: TossJobCategory) -> ScrapedData:
        """íŠ¹ì • ì§êµ°ì˜ ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í•‘"""
        cache_path = self.data_dir / f"scraped_{category.value.lower().replace(' ', '_')}.json"

        # ìºì‹œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if cache_path.exists() and not self.config.force_scrape:
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                logger.info(f"ğŸ“¦ ìºì‹œëœ {category.value} ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì‚¬ìš©")
                return ScrapedData.from_dict(data)
            except Exception as e:
                logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ìƒˆë¡œ ìŠ¤í¬ë˜í•‘
        scraped_data = await self.scraper.scrape_positions_by_category(
            category, headless=self.config.headless
        )

        # ìºì‹œì— ì €ì¥
        if scraped_data.positions:
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(scraped_data.to_dict(), f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ {category.value} ìŠ¤í¬ë˜í•‘ ë°ì´í„° ìºì‹œ ì €ì¥")

        return scraped_data

    def _get_position_name(self, category: TossJobCategory) -> str:
        """ì§êµ° ì¹´í…Œê³ ë¦¬ì—ì„œ í¬ì§€ì…˜ëª… ìƒì„±"""
        mapping = {
            TossJobCategory.BACKEND: "Server Developer",
            TossJobCategory.APP: "App Developer",
            TossJobCategory.FRONTEND: "Frontend Developer",
            TossJobCategory.FULLSTACK: "Full Stack Developer",
            TossJobCategory.INFRA: "DevOps Engineer",
            TossJobCategory.QA: "QA Engineer",
            TossJobCategory.DEVICE: "Embedded Developer",
        }
        return mapping.get(category, "Developer")

    def _get_recommended_job_urls(
        self,
        primary: TossJobCategory,
        secondary: list[TossJobCategory]
    ) -> list[str]:
        """ì¶”ì²œ ì±„ìš©ê³µê³  URL ëª©ë¡ ìƒì„±"""
        urls = []

        # ì£¼ ì§êµ° URL
        primary_url = self.scraper.get_first_job_url_for_category(primary)
        if primary_url:
            urls.append(primary_url)

        # ë¶€ ì§êµ° URL (ìµœëŒ€ 2ê°œ)
        for cat in secondary[:2]:
            url = self.scraper.get_first_job_url_for_category(cat)
            if url and url not in urls:
                urls.append(url)

        return urls

    def format_result(self, result: EvaluationResult) -> str:
        """í‰ê°€ ê²°ê³¼ í¬ë§·íŒ…

        Args:
            result: í‰ê°€ ê²°ê³¼

        Returns:
            í¬ë§·íŒ…ëœ ë¬¸ìì—´
        """
        return self.evaluator.format_result(result)

    @property
    def is_initialized(self) -> bool:
        """ì´ˆê¸°í™” ì™„ë£Œ ì—¬ë¶€"""
        return self._initialized

    @property
    def scraped_data(self) -> Optional[ScrapedData]:
        """ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°"""
        return self._scraped_data

    @property
    def generated_prompt(self) -> Optional[GeneratedPrompt]:
        """ìƒì„±ëœ í”„ë¡¬í”„íŠ¸"""
        return self._generated_prompt

    def get_status(self) -> dict:
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ

        Returns:
            ìƒíƒœ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        status = {
            "initialized": self._initialized,
            "data_dir": str(self.data_dir),
            "ai_provider": self.config.ai_provider,
            "target_position": self.config.target_position,
        }

        if self._scraped_data:
            status["scraped_data"] = {
                "positions_count": len(self._scraped_data.positions),
                "scraped_at": self._scraped_data.scraped_at.isoformat(),
                "content_hash": self._scraped_data.content_hash,
            }

        if self._generated_prompt:
            status["generated_prompt"] = {
                "source_hash": self._generated_prompt.source_hash,
                "generated_at": self._generated_prompt.generated_at.isoformat(),
                "prompt_length": len(self._generated_prompt.prompt),
            }

        return status


async def run_workflow(
    resume_path: str,
    position: str = "Server Developer",
    ai_provider: str = "claude",
    force_scrape: bool = False,
    force_regenerate: bool = False,
    headless: bool = True,
) -> EvaluationResult:
    """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜

    Args:
        resume_path: ì´ë ¥ì„œ íŒŒì¼ ê²½ë¡œ
        position: ì§€ì› í¬ì§€ì…˜
        ai_provider: AI ì œê³µì
        force_scrape: ê°•ì œ ìŠ¤í¬ë˜í•‘ ì—¬ë¶€
        force_regenerate: ê°•ì œ í”„ë¡¬í”„íŠ¸ ì¬ìƒì„± ì—¬ë¶€
        headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€

    Returns:
        EvaluationResult: í‰ê°€ ê²°ê³¼
    """
    config = WorkflowConfig(
        ai_provider=ai_provider,
        force_scrape=force_scrape,
        force_regenerate=force_regenerate,
        headless=headless,
    )

    workflow = ResumeEvaluationWorkflow(config)
    await workflow.initialize()

    result = await workflow.evaluate_resume_file(resume_path, position)
    print(workflow.format_result(result))

    return result


async def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    logging.basicConfig(level=logging.INFO)

    # ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”ë§Œ í…ŒìŠ¤íŠ¸
    config = WorkflowConfig(
        ai_provider="claude",
        force_scrape=False,
        force_regenerate=False,
        headless=True,
    )

    workflow = ResumeEvaluationWorkflow(config)
    success = await workflow.initialize()

    if success:
        print("\nğŸ“Š ì›Œí¬í”Œë¡œìš° ìƒíƒœ:")
        status = workflow.get_status()
        for key, value in status.items():
            print(f"  {key}: {value}")


if __name__ == "__main__":
    asyncio.run(main())
