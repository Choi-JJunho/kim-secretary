"""ì¹´í˜24 ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼ (Playwright ê¸°ë°˜)"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional

from playwright.async_api import async_playwright, Page

from .models import JobRequirement, ScrapedData, PositionCategory, Cafe24JobCategory

logger = logging.getLogger(__name__)


class Cafe24JobScraper:
    """ì¹´í˜24 ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼"""

    BASE_URL = "https://www.cafe24corp.com/recruit/jobs"

    def __init__(self, data_dir: str = "data/resume_evaluator/cafe24"):
        """
        Args:
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.scraped_data_path = self.data_dir / "scraped_positions.json"

    async def scrape_positions_by_category(
        self,
        category: Cafe24JobCategory,
        headless: bool = True
    ) -> ScrapedData:
        """íŠ¹ì • ì§êµ°ì˜ í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘

        Args:
            category: ì§êµ° ì¹´í…Œê³ ë¦¬
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€

        Returns:
            ScrapedData: ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°
        """
        logger.info(f"ğŸš€ ì¹´í˜24 {category.value} í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

        positions = []

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=headless)
            page = await browser.new_page()

            try:
                # ì±„ìš©ê³µê³  ëª©ë¡ í˜ì´ì§€ë¡œ ì´ë™
                await page.goto(self.BASE_URL)
                await page.wait_for_timeout(2000)

                # ëª¨ë“  í˜ì´ì§€ì˜ ì±„ìš©ê³µê³  ìˆ˜ì§‘
                page_num = 1
                while True:
                    logger.info(f"ğŸ“„ í˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘ ì¤‘...")

                    # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì±„ìš©ê³µê³  ì¶”ì¶œ
                    page_positions = await self._scrape_page_positions(page, category)
                    positions.extend(page_positions)

                    # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
                    next_page = await self._goto_next_page(page, page_num + 1)
                    if not next_page:
                        break
                    page_num += 1
                    await page.wait_for_timeout(1000)

            except Exception as e:
                logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            finally:
                await browser.close()

        scraped_data = ScrapedData(
            positions=positions,
            scraped_at=datetime.now(),
            source_url=f"{self.BASE_URL}?category={category.value}",
        )

        logger.info(f"âœ… ì´ {len(positions)}ê°œ {category.value} í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        return scraped_data

    async def _scrape_page_positions(
        self,
        page: Page,
        category: Cafe24JobCategory
    ) -> list[JobRequirement]:
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ì±„ìš©ê³µê³  ì¶”ì¶œ

        Args:
            page: Playwright Page ê°ì²´
            category: í•„í„°ë§í•  ì§êµ° ì¹´í…Œê³ ë¦¬

        Returns:
            JobRequirement ë¦¬ìŠ¤íŠ¸
        """
        category_filter = category.value if category != Cafe24JobCategory.ALL else None

        data = await page.evaluate("""
            (categoryFilter) => {
                const allRows = document.querySelectorAll('table tbody tr');
                const jobs = [];

                for (let i = 0; i < allRows.length; i += 2) {
                    const jobRow = allRows[i];
                    const detailRow = allRows[i + 1];

                    if (!jobRow || !detailRow) continue;
                    if (!detailRow.classList.contains('fieldDetail')) continue;

                    const cells = jobRow.querySelectorAll('td');
                    if (cells.length < 3) continue;

                    const jobCategory = cells[0]?.textContent?.trim();
                    const title = cells[1]?.textContent?.trim();

                    // ì¹´í…Œê³ ë¦¬ í•„í„°ë§
                    if (categoryFilter && jobCategory !== categoryFilter) continue;

                    const detailText = detailRow.querySelector('td')?.textContent || '';

                    // ì„¹ì…˜ë³„ íŒŒì‹±
                    const workMatch = detailText.match(/â– \\s*ì—…ë¬´ë‚´ìš©([\\s\\S]*?)(?=â– |$)/);
                    const reqMatch = detailText.match(/â– \\s*ìê²©ìš”ê±´([\\s\\S]*?)(?=â– |$)/);
                    const prefMatch = detailText.match(/â– \\s*ìš°ëŒ€ìš”ê±´([\\s\\S]*?)(?=â– |$)/);

                    const parseItems = (text) => {
                        if (!text) return [];
                        return text.split(/\\n/)
                            .map(s => s.replace(/^\\s*-\\s*/, '').trim())
                            .filter(s => s && s.length > 2 && !s.startsWith('â– ') && !s.includes('ì§€ì›í•˜ê¸°'));
                    };

                    jobs.push({
                        category: jobCategory,
                        title: title,
                        responsibilities: parseItems(workMatch?.[1]),
                        requirements: parseItems(reqMatch?.[1]),
                        preferred: parseItems(prefMatch?.[1])
                    });
                }

                return jobs;
            }
        """, category_filter)

        positions = []
        for item in data:
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            pos_category = self._map_to_position_category(item["category"])

            position = JobRequirement(
                title=item["title"],
                company="ì¹´í˜24",
                requirements=item["requirements"],
                preferred=item["preferred"],
                tech_stack=[],  # ì¹´í˜24ëŠ” ë³„ë„ ê¸°ìˆ ìŠ¤íƒ ì„¹ì…˜ ì—†ìŒ
                responsibilities=item["responsibilities"],
                job_id=f"cafe24_{hash(item['title']) % 10000:04d}",
                category=pos_category,
                scraped_at=datetime.now(),
            )
            positions.append(position)
            logger.info(f"âœ… {position.title} ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")

        return positions

    async def _goto_next_page(self, page: Page, next_page_num: int) -> bool:
        """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™

        Args:
            page: Playwright Page ê°ì²´
            next_page_num: ì´ë™í•  í˜ì´ì§€ ë²ˆí˜¸

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë‹¤ìŒ í˜ì´ì§€ ë§í¬ ì°¾ê¸°
            next_link = page.locator(f'ul.paging li a:has-text("{next_page_num}")')
            if await next_link.count() > 0:
                await next_link.click()
                await page.wait_for_timeout(1500)
                return True
            return False
        except Exception:
            return False

    def _map_to_position_category(self, cafe24_category: str) -> PositionCategory:
        """ì¹´í˜24 ì¹´í…Œê³ ë¦¬ë¥¼ PositionCategoryë¡œ ë§¤í•‘"""
        mapping = {
            "ê¸°íš/ìš´ì˜": PositionCategory.OTHER,  # PM/ê¸°íšì€ ë³„ë„ ì¹´í…Œê³ ë¦¬
            "ê°œë°œ/ì‹œìŠ¤í…œ": PositionCategory.BACKEND,
            "ë””ìì¸": PositionCategory.OTHER,
            "ë§ˆì¼€íŒ…": PositionCategory.OTHER,
            "ê²½ì˜ì§€ì›": PositionCategory.OTHER,
            "ì œíœ´/ì˜ì—…": PositionCategory.OTHER,
            "ê³ ê°ì§€ì›": PositionCategory.OTHER,
            "ê¸°íƒ€": PositionCategory.OTHER,
        }
        return mapping.get(cafe24_category, PositionCategory.OTHER)

    def save_scraped_data(self, data: ScrapedData) -> None:
        """ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì €ì¥"""
        with open(self.scraped_data_path, "w", encoding="utf-8") as f:
            json.dump(data.to_dict(), f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.scraped_data_path}")

    def load_scraped_data(self) -> Optional[ScrapedData]:
        """ì €ì¥ëœ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ"""
        if not self.scraped_data_path.exists():
            return None

        try:
            with open(self.scraped_data_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            return ScrapedData.from_dict(data)
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def get_content_hash(self) -> Optional[str]:
        """ì €ì¥ëœ ë°ì´í„°ì˜ content_hash ë°˜í™˜"""
        data = self.load_scraped_data()
        return data.content_hash if data else None

    def has_changes(self, new_data: ScrapedData) -> bool:
        """ë°ì´í„° ë³€ê²½ ì—¬ë¶€ í™•ì¸"""
        old_hash = self.get_content_hash()
        if old_hash is None:
            return True
        return old_hash != new_data.content_hash


async def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    logging.basicConfig(level=logging.INFO)

    scraper = Cafe24JobScraper()
    # ê¸°íš/ìš´ì˜ ì§êµ°ë§Œ ìŠ¤í¬ë˜í•‘
    data = await scraper.scrape_positions_by_category(
        Cafe24JobCategory.PLANNING,
        headless=True
    )

    print(f"\nğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼: {len(data.positions)}ê°œ í¬ì§€ì…˜")
    for pos in data.positions:
        print(f"  - {pos.title}")
        print(f"    ì—…ë¬´: {len(pos.responsibilities)}ê°œ í•­ëª©")
        print(f"    ìê²©ìš”ê±´: {len(pos.requirements)}ê°œ í•­ëª©")
        print(f"    ìš°ëŒ€: {len(pos.preferred)}ê°œ í•­ëª©")

    scraper.save_scraped_data(data)


if __name__ == "__main__":
    asyncio.run(main())
