"""ì¹´í˜24 ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼ (Playwright ê¸°ë°˜) - ë™ì  íƒìƒ‰"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional

from playwright.async_api import async_playwright, Page

from .models import JobRequirement, ScrapedData, PositionCategory, Cafe24JobCategory

logger = logging.getLogger(__name__)


class Cafe24JobScraper:
    """ì¹´í˜24 ì±„ìš©ê³µê³  ìŠ¤í¬ë˜í¼ - ë™ì  íƒìƒ‰"""

    BASE_URL = "https://www.cafe24corp.com/recruit/jobs"

    def __init__(self, data_dir: str = "data/resume_evaluator/cafe24"):
        """
        Args:
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.scraped_data_path = self.data_dir / "scraped_positions.json"

    async def _goto_next_page(self, page: Page, next_page_num: int) -> bool:
        """ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            next_link = page.locator(f'ul.paging li a:has-text("{next_page_num}")')
            if await next_link.count() > 0:
                await next_link.click()
                await page.wait_for_timeout(1500)
                return True
            return False
        except Exception:
            return False

    async def scrape_positions_by_category(
        self,
        category: Cafe24JobCategory,
        headless: bool = True,
        max_jobs: int = 10
    ) -> ScrapedData:
        """íŠ¹ì • ì§êµ°ì˜ í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ (ë™ì  íƒìƒ‰)

        Args:
            category: ì§êµ° ì¹´í…Œê³ ë¦¬
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
            max_jobs: ìµœëŒ€ ìŠ¤í¬ë˜í•‘í•  ê³µê³  ìˆ˜

        Returns:
            ScrapedData: ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°
        """
        logger.info(f"ğŸš€ ì¹´í˜24 {category.value} í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

        positions = []

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=headless)
            page = await browser.new_page()

            try:
                # ì±„ìš© ëª©ë¡ í˜ì´ì§€ë¡œ ì´ë™
                await page.goto(self.BASE_URL)
                await page.wait_for_timeout(2000)

                # ëª¨ë“  í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ìŠ¤í¬ë˜í•‘
                category_filter = category.value if category != Cafe24JobCategory.ALL else None
                page_num = 1
                scraped_count = 0

                while scraped_count < max_jobs:
                    logger.info(f"ğŸ“„ í˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘ ì¤‘...")

                    # í˜„ì¬ í˜ì´ì§€ì—ì„œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ê³µê³  ìŠ¤í¬ë˜í•‘
                    page_positions = await self._scrape_page_positions_with_details(
                        page, category_filter, max_jobs - scraped_count
                    )
                    positions.extend(page_positions)
                    scraped_count += len(page_positions)

                    if scraped_count >= max_jobs:
                        break

                    # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                    if not await self._goto_next_page(page, page_num + 1):
                        break
                    page_num += 1
                    await page.wait_for_timeout(1000)

            except Exception as e:
                logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            finally:
                await browser.close()

        scraped_data = ScrapedData(
            positions=positions,
            scraped_at=datetime.now(),
            source_url=f"{self.BASE_URL}?category={category.value}",
        )

        logger.info(f"âœ… ì´ {len(positions)}ê°œ {category.value} í¬ì§€ì…˜ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        return scraped_data

    async def _scrape_page_positions_with_details(
        self,
        page: Page,
        category_filter: Optional[str],
        max_count: int
    ) -> list[JobRequirement]:
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ê³µê³  ëª©ë¡ê³¼ ìƒì„¸ ì •ë³´ë¥¼ í•¨ê»˜ ìŠ¤í¬ë˜í•‘

        Args:
            page: Playwright Page ê°ì²´
            category_filter: í•„í„°ë§í•  ì¹´í…Œê³ ë¦¬ (Noneì´ë©´ ì „ì²´)
            max_count: ìµœëŒ€ ìŠ¤í¬ë˜í•‘í•  ê³µê³  ìˆ˜

        Returns:
            JobRequirement ë¦¬ìŠ¤íŠ¸
        """
        # JavaScriptë¡œ í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ê³µê³  ì •ë³´ ì¶”ì¶œ (ìƒì„¸ ì •ë³´ í¬í•¨)
        data = await page.evaluate("""
            (categoryFilter) => {
                const allRows = document.querySelectorAll('table tbody tr');
                const jobs = [];

                for (let i = 0; i < allRows.length; i += 2) {
                    const jobRow = allRows[i];
                    const detailRow = allRows[i + 1];

                    if (!jobRow || !detailRow) continue;
                    if (!detailRow.classList.contains('fieldDetail')) continue;

                    const cells = jobRow.querySelectorAll('td');
                    if (cells.length < 3) continue;

                    const jobCategory = cells[0]?.textContent?.trim();
                    const title = cells[1]?.textContent?.trim();

                    // ì¹´í…Œê³ ë¦¬ í•„í„°ë§
                    if (categoryFilter && jobCategory !== categoryFilter) continue;

                    const detailText = detailRow.querySelector('td')?.textContent || '';

                    // ì„¹ì…˜ë³„ íŒŒì‹±
                    const parseSection = (text, sectionName) => {
                        const regex = new RegExp('â– \\\\s*' + sectionName + '([\\\\s\\\\S]*?)(?=â– |$)');
                        const match = text.match(regex);
                        if (!match) return [];

                        return match[1]
                            .split(/\\n/)
                            .map(s => s.replace(/^\\s*-\\s*/, '').trim())
                            .filter(s => s && s.length > 2 && !s.startsWith('â– ') && !s.includes('ì§€ì›í•˜ê¸°'));
                    };

                    jobs.push({
                        category: jobCategory,
                        title: title,
                        responsibilities: parseSection(detailText, 'ì—…ë¬´ë‚´ìš©'),
                        requirements: parseSection(detailText, 'ìê²©ìš”ê±´'),
                        preferred: parseSection(detailText, 'ìš°ëŒ€ìš”ê±´')
                    });
                }

                return jobs;
            }
        """, category_filter)

        positions = []
        for item in data[:max_count]:
            if not item.get("requirements"):
                continue

            pos_category = self._map_to_position_category(item["category"])

            position = JobRequirement(
                title=item["title"],
                company="ì¹´í˜24",
                requirements=item["requirements"],
                preferred=item.get("preferred", []),
                tech_stack=[],
                responsibilities=item.get("responsibilities", []),
                job_id=f"cafe24_{hash(item['title']) % 100000:05d}",
                category=pos_category,
                scraped_at=datetime.now(),
            )
            positions.append(position)
            logger.info(f"âœ… {position.title} ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")

        return positions

    def _map_to_position_category(self, cafe24_category: str) -> PositionCategory:
        """ì¹´í˜24 ì¹´í…Œê³ ë¦¬ë¥¼ PositionCategoryë¡œ ë§¤í•‘"""
        mapping = {
            "ê¸°íš/ìš´ì˜": PositionCategory.OTHER,  # PM/ê¸°íšì€ ë³„ë„ ì¹´í…Œê³ ë¦¬
            "ê°œë°œ/ì‹œìŠ¤í…œ": PositionCategory.BACKEND,
            "ë””ìì¸": PositionCategory.OTHER,
            "ë§ˆì¼€íŒ…": PositionCategory.OTHER,
            "ê²½ì˜ì§€ì›": PositionCategory.OTHER,
            "ì œíœ´/ì˜ì—…": PositionCategory.OTHER,
            "ê³ ê°ì§€ì›": PositionCategory.OTHER,
            "ê¸°íƒ€": PositionCategory.OTHER,
        }
        return mapping.get(cafe24_category, PositionCategory.OTHER)

    def get_available_categories(self) -> list[Cafe24JobCategory]:
        """ìŠ¤í¬ë˜í•‘ ê°€ëŠ¥í•œ ì§êµ° ëª©ë¡ ë°˜í™˜"""
        return list(Cafe24JobCategory)

    def save_scraped_data(self, data: ScrapedData) -> None:
        """ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì €ì¥"""
        with open(self.scraped_data_path, "w", encoding="utf-8") as f:
            json.dump(data.to_dict(), f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.scraped_data_path}")

    def load_scraped_data(self) -> Optional[ScrapedData]:
        """ì €ì¥ëœ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ"""
        if not self.scraped_data_path.exists():
            return None

        try:
            with open(self.scraped_data_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            return ScrapedData.from_dict(data)
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def get_content_hash(self) -> Optional[str]:
        """ì €ì¥ëœ ë°ì´í„°ì˜ content_hash ë°˜í™˜"""
        data = self.load_scraped_data()
        return data.content_hash if data else None

    def has_changes(self, new_data: ScrapedData) -> bool:
        """ë°ì´í„° ë³€ê²½ ì—¬ë¶€ í™•ì¸"""
        old_hash = self.get_content_hash()
        if old_hash is None:
            return True
        return old_hash != new_data.content_hash


async def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    logging.basicConfig(level=logging.INFO)

    scraper = Cafe24JobScraper()

    # ê¸°íš/ìš´ì˜ ì§êµ° ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª ì¹´í˜24 ê¸°íš/ìš´ì˜ ì§êµ° ë™ì  ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸")
    data = await scraper.scrape_positions_by_category(
        Cafe24JobCategory.PLANNING,
        headless=True,
        max_jobs=3
    )

    print(f"\nğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼: {len(data.positions)}ê°œ í¬ì§€ì…˜")
    for pos in data.positions:
        print(f"  - {pos.title}")
        print(f"    ì—…ë¬´: {len(pos.responsibilities)}ê°œ í•­ëª©")
        print(f"    ìê²©ìš”ê±´: {len(pos.requirements)}ê°œ í•­ëª©")
        print(f"    ìš°ëŒ€: {len(pos.preferred)}ê°œ í•­ëª©")

    scraper.save_scraped_data(data)


if __name__ == "__main__":
    asyncio.run(main())
