"""Ollama ë¡œì»¬ AI ì œê³µì"""

import logging
import os
from typing import Optional

import httpx

from .base import AIProvider

logger = logging.getLogger(__name__)


class OllamaProvider(AIProvider):
  """Ollama ë¡œì»¬ AI ì œê³µì"""

  def __init__(self):
    """Ollama ì œê³µì ì´ˆê¸°í™”"""
    self.base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    self.model_name = os.getenv("OLLAMA_MODEL", "llama3.2")
    self.validate_config()
    logger.info(f"âœ… Ollama provider initialized: {self.model_name}")

  def validate_config(self) -> bool:
    """Ollama ì„¤ì • ê²€ì¦"""
    # Just validate URL format, actual connectivity is checked on use
    if not self.base_url.startswith("http"):
      raise ValueError(
          f"Invalid OLLAMA_BASE_URL: {self.base_url}. "
          "Must start with http:// or https://"
      )
    return True

  async def generate(
      self,
      prompt: str,
      system_prompt: Optional[str] = None,
      **kwargs
  ) -> str:
    """Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    try:
      logger.info("ğŸ¤– Ollama ì‘ë‹µ ìƒì„± ì¤‘...")

      async with httpx.AsyncClient(timeout=60.0) as client:
        payload = {
          "model": self.model_name,
          "prompt": prompt,
          "stream": False,
          **kwargs
        }

        if system_prompt:
          payload["system"] = system_prompt

        response = await client.post(
            f"{self.base_url}/api/generate",
            json=payload
        )
        response.raise_for_status()

        result = response.json()["response"]
        logger.info(f"âœ… Ollama ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(result)}ì)")
        return result

    except httpx.ConnectError:
      logger.error(
          f"âŒ {self.base_url} ì—ì„œ Ollamaì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
      )
      raise
    except Exception as e:
      logger.error(f"âŒ Ollama ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
      raise
